{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade snowflake-connector-python\n",
    "# pip install --upgrade snowflake-sqlalchemy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "\n",
    "1. **Extract** (you can manually extract all the data files in the provided `data.zip` file) and load the 41 comma-delimited purchases data files and form a single table of purchases data (you should load the 41 CSV files into a Snowflake stage and then move the data from the stage to a Snowflake table):\n",
    "   \n",
    "   a. Preferably follow these guidelines when staging the files (this staging approach does not make sense for our data as the files are small, but it is good practice if you have more data and if the data is loaded over time).\n",
    "   \n",
    "   b. Use Python to automate the `PUT` process, e.g., use `glob` to iterate through and `PUT` all purchases files automatically.\n",
    "   \n",
    "   c. You can examine the data in the stage using regular SQL statements, but where columns are referred to using the positional number of the column preceded by `$`, e.g., `SELECT $.1, $.3 FROM...` selects the first and second column in the staged data: [Snowflake Staging Documentation](https://docs.snowflake.com/user-guide/querying-stage).\n",
    "   \n",
    "   d. `COPY INTO` is generally preferred over `INSERT INTO` (this applies to the entire project).\n",
    "   \n",
    "   e. To the extent possible, perform transformations such as selecting columns and setting data types during the `COPY INTO` process. There are a number of columns that are not needed in the project. You can exclude columns that appear to not have any useful information (e.g., the same value on each row, only null values, etc.). You can also exclude columns that you do not need for the project (look through the instructions and try to determine which columns will be needed. If you realize later that you excluded columns that you need, then simply come back to this code and change it to include the additional column(s) that are causing errors).\n",
    "\n",
    "   f.\tIf you have multiple steps in your code that are needed for moving data from source into the final table and the final output is not as expected (e.g., you can count the number of rows in the raw data and then verify that you have the same number of rows in the final Snowflake table), then try to troubleshoot your code by verifying which step in your process does not produce the expect results. This will for example require examining staged data. Start at the beginning when doing this. When you have located the step that does not produce the expected then start troubleshooting this step in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='wbkennedy',\n",
    "    password='Brandon92!',\n",
    "    account='nulgoll-hsb06466',\n",
    "    warehouse='final_project_warehouse',\n",
    "    database='final_project_db',\n",
    "    schema='final_project_schema'\n",
    ")\n",
    "\n",
    "cs = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.connector.cursor.SnowflakeCursor at 0x7fc4ba638410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a virtual warehouse (virtual warehouses contain the compute resources that are required to perform queries and DML operations with Snowflake)\n",
    "cs.execute(\"CREATE WAREHOUSE IF NOT EXISTS final_project_warehouse\")\n",
    "\n",
    "# create a database\n",
    "cs.execute(\"CREATE DATABASE IF NOT EXISTS final_project_db\")\n",
    "\n",
    "# create schema\n",
    "cs.execute(\"CREATE SCHEMA IF NOT EXISTS final_project_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 2020-2.csv to Snowflake stage\n",
      "Uploaded 2021-3.csv to Snowflake stage\n",
      "Uploaded 2020-5.csv to Snowflake stage\n",
      "Uploaded 2021-11.csv to Snowflake stage\n",
      "Uploaded 2022-1.csv to Snowflake stage\n",
      "Uploaded 2019-8.csv to Snowflake stage\n",
      "Uploaded 2021-7.csv to Snowflake stage\n",
      "Uploaded 2021-10.csv to Snowflake stage\n",
      "Uploaded 2019-6.csv to Snowflake stage\n",
      "Uploaded 2021-8.csv to Snowflake stage\n",
      "Uploaded 2020-10.csv to Snowflake stage\n",
      "Uploaded 2020-8.csv to Snowflake stage\n",
      "Uploaded 2021-1.csv to Snowflake stage\n",
      "Uploaded 2019-4.csv to Snowflake stage\n",
      "Uploaded 2019-12.csv to Snowflake stage\n",
      "Uploaded 2022-4.csv to Snowflake stage\n",
      "Uploaded 2020-11.csv to Snowflake stage\n",
      "Uploaded 2019-10.csv to Snowflake stage\n",
      "Uploaded 2020-12.csv to Snowflake stage\n",
      "Uploaded 2020-3.csv to Snowflake stage\n",
      "Uploaded 2020-1.csv to Snowflake stage\n",
      "Uploaded 2019-2.csv to Snowflake stage\n",
      "Uploaded 2019-11.csv to Snowflake stage\n",
      "Uploaded 2021-2.csv to Snowflake stage\n",
      "Uploaded 2019-3.csv to Snowflake stage\n",
      "Uploaded 2019-9.csv to Snowflake stage\n",
      "Uploaded 2021-12.csv to Snowflake stage\n",
      "Uploaded 2021-5.csv to Snowflake stage\n",
      "Uploaded 2020-4.csv to Snowflake stage\n",
      "Uploaded 2022-5.csv to Snowflake stage\n",
      "Uploaded 2020-6.csv to Snowflake stage\n",
      "Uploaded 2019-7.csv to Snowflake stage\n",
      "Uploaded 2021-9.csv to Snowflake stage\n",
      "Uploaded 2022-2.csv to Snowflake stage\n",
      "Uploaded 2019-1.csv to Snowflake stage\n",
      "Uploaded 2020-7.csv to Snowflake stage\n",
      "Uploaded 2019-5.csv to Snowflake stage\n",
      "Uploaded 2022-3.csv to Snowflake stage\n",
      "Uploaded 2021-4.csv to Snowflake stage\n",
      "Uploaded 2021-6.csv to Snowflake stage\n",
      "Uploaded 2020-9.csv to Snowflake stage\n",
      "Created table purchases_data\n",
      "Data copied from stage to purchases_data table\n",
      "Removed files from stage purchases_stage\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake stage to hold the files\n",
    "stage_name = \"purchases_stage\"\n",
    "cs.execute(f\"CREATE OR REPLACE STAGE {stage_name}\")\n",
    "\n",
    "# Folder containing the CSV files (your provided path)\n",
    "csv_folder_path = '/home/jovyan/MGTA_464/SQLETLSnowflake/CaseData/Data/Monthly PO Data'\n",
    "\n",
    "# Use glob to iterate through all CSV files in the directory and upload them to the stage\n",
    "for file_path in glob.glob(os.path.join(csv_folder_path, \"*.csv\")):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    # Make sure to escape special characters or use quotes for file paths\n",
    "    file_path_escaped = file_path.replace(\" \", \"\\\\ \")\n",
    "    \n",
    "    # Upload the file to the Snowflake stage\n",
    "    put_query = f\"PUT 'file://{file_path_escaped}' @{stage_name}/{file_name}\"\n",
    "    cs.execute(put_query)\n",
    "    print(f\"Uploaded {file_name} to Snowflake stage\")\n",
    "\n",
    "# Create a target table in Snowflake with proper data types\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE purchases_data (\n",
    "        PurchaseOrderID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        OrderDate DATE,\n",
    "        DeliveryMethodID INTEGER,\n",
    "        ContactPersonID INTEGER,\n",
    "        ExpectedDeliveryDate DATE,\n",
    "        SupplierReference STRING,\n",
    "        IsOrderFinalized BOOLEAN,\n",
    "        Comments STRING,\n",
    "        InternalComments STRING,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen TIMESTAMP,\n",
    "        PurchaseOrderLineID INTEGER,\n",
    "        StockItemID INTEGER,\n",
    "        OrderedOuters INTEGER,\n",
    "        Description STRING,\n",
    "        ReceivedOuters INTEGER,\n",
    "        PackageTypeID INTEGER,\n",
    "        ExpectedUnitPricePerOuter FLOAT,\n",
    "        LastReceiptDate DATE,\n",
    "        IsOrderLineFinalized BOOLEAN,\n",
    "        Right_LastEditedBy INTEGER,\n",
    "        Right_LastEditedWhen TIMESTAMP\n",
    "    );\n",
    "\"\"\")\n",
    "print(\"Created table purchases_data\")\n",
    "\n",
    "# Use COPY INTO to move data from the stage to the Snowflake table, with correct formatting and error handling\n",
    "copy_into_query = f\"\"\"\n",
    "COPY INTO purchases_data\n",
    "FROM @{stage_name}\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV',\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',\n",
    "    SKIP_HEADER = 1,\n",
    "    DATE_FORMAT = 'MM/DD/YYYY',\n",
    "    TIMESTAMP_FORMAT = 'MM/DD/YYYY HH24:MI'\n",
    ")\n",
    "ON_ERROR = 'CONTINUE';\n",
    "\"\"\"\n",
    "cs.execute(copy_into_query)\n",
    "print(\"Data copied from stage to purchases_data table\")\n",
    "\n",
    "# Remove the files from the stage after loading them to the table\n",
    "cs.execute(f\"REMOVE @{stage_name}\")\n",
    "print(f\"Removed files from stage {stage_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "\n",
    "2. **Create a calculated field** that shows purchase order totals, i.e., for each order, sum the line-item amounts (defined as `ReceivedOuters * ExpectedUnitPricePerOuter`), and name this field `POAmount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added POAmount column\n",
      "Updated POAmount values\n"
     ]
    }
   ],
   "source": [
    "# Add the POAmount column\n",
    "cs.execute(\"\"\"\n",
    "    ALTER TABLE purchases_data\n",
    "    ADD COLUMN POAmount FLOAT;\n",
    "\"\"\")\n",
    "print(\"Added POAmount column\")\n",
    "\n",
    "# Update the table to calculate POAmount\n",
    "cs.execute(\"\"\"\n",
    "    UPDATE purchases_data\n",
    "    SET POAmount = ReceivedOuters * ExpectedUnitPricePerOuter;\n",
    "\"\"\")\n",
    "print(\"Updated POAmount values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:\n",
    "\n",
    "3. **Load the supplier invoice XML data** (you will again first stage the data and then move it into a table):\n",
    "\n",
    "   a. Shred the data into a table (preferably in the `COPY INTO` process) where each row corresponds to a single invoice.\n",
    "   \n",
    "   b. Make sure to examine the structure of the XML file and also try different functions such as `GETXML`, `GET`, `PARSE_XML`, `FLATTEN`, etc.\n",
    "   \n",
    "   c. When building your query to shred the data, try to keep it as simple as possible at first and only attempt to extract a single element or only try a single SQL clause or function to see what it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'xml_stage' created or exists.\n",
      "XML file format 'xml_file_format' created.\n",
      "XML file uploaded to stage.\n",
      "Table 'RawXMLData' created or replaced successfully.\n",
      "XML data loaded into 'RawXMLData' successfully.\n",
      "Table 'SupplierInvoices' created or replaced successfully.\n",
      "Transformed and loaded XML data into 'SupplierInvoices' successfully using XMLGET().\n"
     ]
    }
   ],
   "source": [
    "# Create a stage for uploading XML data\n",
    "cs.execute(\"CREATE OR REPLACE STAGE xml_stage\")\n",
    "print(\"Stage 'xml_stage' created or exists.\")\n",
    "\n",
    "# Create an XML file format in Snowflake\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT xml_file_format \n",
    "    TYPE = 'XML' \n",
    "    STRIP_OUTER_ELEMENT = TRUE\n",
    "\"\"\")\n",
    "print(\"XML file format 'xml_file_format' created.\")\n",
    "\n",
    "# Upload the XML file to the Snowflake stage\n",
    "xml_file_path = \"/home/jovyan/MGTA_464/SQLETLSnowflake/CaseData/Data/Supplier Transactions XML.xml\"\n",
    "put_command = f\"PUT 'file://{xml_file_path}' @xml_stage auto_compress=true\"\n",
    "cs.execute(put_command)\n",
    "print(\"XML file uploaded to stage.\")\n",
    "\n",
    "# Create a table to store the raw XML data\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE RawXMLData (\n",
    "        xml_column VARIANT\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'RawXMLData' created or replaced successfully.\")\n",
    "\n",
    "# Load the raw XML data into the table\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        COPY INTO RawXMLData \n",
    "        FROM @xml_stage \n",
    "        FILE_FORMAT = (FORMAT_NAME = 'xml_file_format') \n",
    "        ON_ERROR = 'CONTINUE'\n",
    "    \"\"\")\n",
    "    print(\"XML data loaded into 'RawXMLData' successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading XML data: {e}\")\n",
    "\n",
    "# Create the SupplierInvoices table\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE SupplierInvoices (\n",
    "        SupplierTransactionID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        TransactionTypeID INTEGER,\n",
    "        PurchaseOrderID INTEGER,\n",
    "        PaymentMethodID INTEGER,\n",
    "        SupplierInvoiceNumber STRING,\n",
    "        TransactionDate DATE,\n",
    "        AmountExcludingTax FLOAT,\n",
    "        TaxAmount FLOAT,\n",
    "        TransactionAmount FLOAT,\n",
    "        OutstandingBalance FLOAT,\n",
    "        FinalizationDate DATE,\n",
    "        IsFinalized BOOLEAN,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen TIMESTAMP_NTZ\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'SupplierInvoices' created or replaced successfully.\")\n",
    "\n",
    "# Transform and load data from RawXMLData to SupplierInvoices using XMLGET()\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        INSERT INTO SupplierInvoices\n",
    "        SELECT \n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'SupplierTransactionID'):\"$\"::STRING) AS SupplierTransactionID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'SupplierID'):\"$\"::STRING) AS SupplierID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TransactionTypeID'):\"$\"::STRING) AS TransactionTypeID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'PurchaseOrderID'):\"$\"::STRING) AS PurchaseOrderID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'PaymentMethodID'):\"$\"::STRING) AS PaymentMethodID,\n",
    "            XMLGET(xml_column, 'SupplierInvoiceNumber'):\"$\"::STRING AS SupplierInvoiceNumber,\n",
    "            TRY_TO_DATE(XMLGET(xml_column, 'TransactionDate'):\"$\"::STRING, 'YYYY-MM-DD') AS TransactionDate,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'AmountExcludingTax'):\"$\"::STRING) AS AmountExcludingTax,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TaxAmount'):\"$\"::STRING) AS TaxAmount,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TransactionAmount'):\"$\"::STRING) AS TransactionAmount,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'OutstandingBalance'):\"$\"::STRING) AS OutstandingBalance,\n",
    "            TRY_TO_DATE(XMLGET(xml_column, 'FinalizationDate'):\"$\"::STRING, 'YYYY-MM-DD') AS FinalizationDate,\n",
    "            TRY_TO_BOOLEAN(XMLGET(xml_column, 'IsFinalized'):\"$\"::STRING) AS IsFinalized,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'LastEditedBy'):\"$\"::STRING) AS LastEditedBy,\n",
    "            TRY_TO_TIMESTAMP_NTZ(XMLGET(xml_column, 'LastEditedWhen'):\"$\"::STRING, 'YYYY-MM-DD HH24:MI:SS.FF') AS LastEditedWhen\n",
    "        FROM RawXMLData\n",
    "    \"\"\")\n",
    "    print(\"Transformed and loaded XML data into 'SupplierInvoices' successfully using XMLGET().\")\n",
    "except Exception as e:\n",
    "    print(f\"Error transforming and loading XML data using XMLGET(): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:\n",
    "\n",
    "4. **Join the purchases data from step 2** and the supplier invoices data from step 3 (only include matching rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5:\n",
    "\n",
    "5. Using the joined data from step 4, create a calculated field that shows the difference between `AmountExcludingTax` and `POAmount`, name this field `invoiced_vs_quoted`, and save the result as a materialized view named `purchase_orders_and_invoices`:\n",
    "   \n",
    "   a. If your version of Snowflake does not support materialized views, then create a table instead using the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'purchase_orders_and_invoices' created.\n"
     ]
    }
   ],
   "source": [
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE purchase_orders_and_invoices AS\n",
    "    SELECT \n",
    "    \n",
    "        (SI.AmountExcludingTax - PD.POAmount) AS invoiced_vs_quoted,\n",
    "    FROM PURCHASES_DATA PD\n",
    "    INNER JOIN SupplierInvoices SI\n",
    "    ON PD.PurchaseOrderID = SI.PurchaseOrderID\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"Table 'purchase_orders_and_invoices' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:\n",
    "\n",
    "6. Manually open the `supplier_case` SQL script (in the SQL editor that you have used in class previously, e.g., VS Code) and run the code to create the `supplier_case` table (you can create the table in `WestCoastImporters` or any other database). Then extract the `supplier_case` data from the Postgres table you just created (do not import the data into Python) by using Python to move the data from Postgres directly to your local drive and then directly into a Snowflake stage.\n",
    "\n",
    "   a. Consider creating a Python function that can take a CSV file path as input and then generate field definitions (field names and datatypes based on the header and data types in the file) that can then be used in a `CREATE TABLE` statement.\n",
    "   \n",
    "   b. You need to use `psycopg2` or a similar Python library to connect to the Postgres database within Python, issue a command to Postgres to have Postgres save the `supplier_case` data to a file, and then use `cs.execute` to move the file to an internal Snowflake stage and eventually into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to supplier_case.csv\n",
      "Uploaded supplier_case.csv to Snowflake stage\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "\n",
    "def export_postgres_to_csv(db_conn_str, query, output_file):\n",
    "    conn = psycopg2.connect(db_conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        cursor.copy_expert(f\"COPY ({query}) TO STDOUT WITH CSV HEADER\", f)\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(f\"Data exported to {output_file}\")\n",
    "\n",
    "# Connection string to your Postgres database\n",
    "postgres_conn_str = \"dbname='WestCoastImporters' user='jovyan' host='127.0.0.1' port='8765' password='postgres'\"\n",
    "\n",
    "# SQL query to extract the data from the table\n",
    "query = \"SELECT * FROM supplier_case\"\n",
    "\n",
    "# Export data to CSV\n",
    "export_postgres_to_csv(postgres_conn_str, query, \"supplier_case.csv\")\n",
    "\n",
    "# Upload the CSV file to the Snowflake stage\n",
    "stage_name = \"supplier_stage\"\n",
    "csv_file = \"supplier_case.csv\"\n",
    "\n",
    "cs.execute(f\"PUT file://{csv_file} @{stage_name}\")\n",
    "print(f\"Uploaded {csv_file} to Snowflake stage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created supplier_case table in Snowflake\n",
      "Data loaded into the supplier_case table\n"
     ]
    }
   ],
   "source": [
    "# Create the table before loading data\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE supplier_case (\n",
    "        supplierid STRING,\n",
    "        suppliername STRING,\n",
    "        suppliercategoryid STRING,\n",
    "        primarycontactpersonid STRING,\n",
    "        alternatecontactpersonid STRING,\n",
    "        deliverymethodid STRING,\n",
    "        postalcityid STRING,\n",
    "        supplierreference STRING,\n",
    "        bankaccountname STRING,\n",
    "        bankaccountbranch STRING,\n",
    "        bankaccountcode STRING,\n",
    "        bankaccountnumber STRING,\n",
    "        bankinternationalcode STRING,\n",
    "        paymentdays INT,\n",
    "        internalcomments STRING,\n",
    "        phonenumber STRING,\n",
    "        faxnumber STRING,\n",
    "        websiteurl STRING,\n",
    "        deliveryaddressline1 STRING,\n",
    "        deliveryaddressline2 STRING,\n",
    "        deliverypostalcode STRING,\n",
    "        deliverylocation STRING,\n",
    "        postaladdressline1 STRING,\n",
    "        postaladdressline2 STRING,\n",
    "        postalpostalcode STRING,\n",
    "        lasteditedby STRING,\n",
    "        validfrom STRING,\n",
    "        validto STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created supplier_case table in Snowflake\")\n",
    "\n",
    "# Define a file format with proper handling for enclosed fields\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT my_csv_format\n",
    "    TYPE = 'CSV'\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n",
    "    SKIP_HEADER = 1\n",
    "    NULL_IF = ('');\n",
    "\"\"\")\n",
    "\n",
    "# Copy data from the Snowflake stage to the table using the custom file format\n",
    "cs.execute(f\"\"\"\n",
    "    COPY INTO supplier_case\n",
    "    FROM @{stage_name}/{csv_file.split('/')[-1]}\n",
    "    FILE_FORMAT = my_csv_format;\n",
    "\"\"\")\n",
    "print(\"Data loaded into the supplier_case table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7:\n",
    "\n",
    "7. **Connect manually to NOAA data using Marketplace**. From inside Snowflake Marketplace (from the home screen, click **Data Products**) search for **NOAA** and then select **Weather & Environment from Cybersyn** (click **Get**). The name of the datasets that you will be using can be accessed in SQL queries running on Snowflake using `cybersyn.noaa_weather_metrics_timeseries` and `cybersyn.noaa_weather_station_index` (`NOAA_WEATHER_METRICS_ATTRIBUTES` additionally contains data definitions that might be helpful). Using this data, extract weather data for each unique zip code in the `supplier_case` table (suppliers can have the same zip code, but you only need to extract weather data for each zip code once).\n",
    "\n",
    "   a. While the weather station data contain zip codes, we will pretend that this table does not have this information and instead use latitude and longitude information to determine which weather station to use for each zip code. The approach used in [this article](https://towardsdatascience.com/noaa-weather-data-in-snowflake-free-20e90ee916ed) can be helpful (note that this is based on a different dataset, but the idea of using latitude and longitude is the same) for finding weather stations closest to each zip code (only use one weather station per zip code). For this to work, you need to find a data file with zip code – geo-location mappings, e.g., from the US Census (the data zip folder on Canvas contains a ZCTA file with this information; in this file `GEOID` is the five-digit ZIP Code, `INTPTLAT` is Latitude, and `INTPTLONG` is Longitude).\n",
    "\n",
    "   b. Create a **materialized view** named `supplier_zip_code_weather` that contains the unique zip codes (`PostalPostalCode`) from the supplier data, date, and daily high temperatures. The view should have three columns: `zip code`, `date`, and `high temperature`, and one row per day and unique supplier zip code. You will not have temperature data for all the suppliers. This is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GEOID      ALAND   AWATER  ALAND_SQMI  AWATER_SQMI   INTPTLAT  \\\n",
      "0  00601  166847909   799292      64.420        0.309  18.180555   \n",
      "1  00602   78546713  4428428      30.327        1.710  18.361945   \n",
      "2  00603   88957333  6276536      34.347        2.423  18.458497   \n",
      "3  00606  114825382    12487      44.334        0.005  18.158327   \n",
      "4  00610   96129350  4310530      37.116        1.664  18.294032   \n",
      "\n",
      "   INTPTLONG                                                                                                                                    \n",
      "0                                         -66.749961                                                                                            \n",
      "1                                         -67.175597                                                                                            \n",
      "2                                         -67.123906                                                                                            \n",
      "3                                         -66.932928                                                                                            \n",
      "4                                         -67.127156                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# Specify 'GEOID' as a string to preserve leading zeros when loading the file\n",
    "df_zip_geo = pd.read_csv('/home/jovyan/MGTA_464/SQLETLSnowflake/CaseData/Data/2021_Gaz_zcta_national.txt', delimiter='\\t', dtype={'GEOID': str})\n",
    "\n",
    "# Check the first few rows of the DataFrame to ensure leading zeros are preserved\n",
    "print(df_zip_geo.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written to /home/jovyan/MGTA_464/SQLETLSnowflake/CaseData/Data/2021_Gaz_zcta_national.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "output_csv_path = '/home/jovyan/MGTA_464/SQLETLSnowflake/CaseData/Data/2021_Gaz_zcta_national.csv'\n",
    "df_zip_geo.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully written to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table zip_geo_mapping with all columns created successfully.\n",
      "Stage zip_geo_stage created successfully.\n",
      "Uploaded 2021_Gaz_zcta_national.csv to Snowflake stage\n",
      "Data copied from stage to zip_geo_mapping table\n",
      "Sample data from zip_geo_mapping:\n",
      "('00601', 166847909.0, 799292.0, 64.42, 0.309, 18.180555, -66.749961)\n",
      "('00602', 78546713.0, 4428428.0, 30.327, 1.71, 18.361945, -67.175597)\n",
      "('00603', 88957333.0, 6276536.0, 34.347, 2.423, 18.458497, -67.123906)\n",
      "('00606', 114825382.0, 12487.0, 44.334, 0.005, 18.158327, -66.932928)\n",
      "('00610', 96129350.0, 4310530.0, 37.116, 1.664, 18.294032, -67.127156)\n",
      "('00611', 27570859.0, 3631.0, 10.645, 0.001, 18.276316, -66.807165)\n",
      "('00612', 197202754.0, 15258391.0, 76.14, 5.891, 18.416727, -66.70009)\n",
      "('00616', 28189755.0, 143499.0, 10.884, 0.055, 18.420412, -66.671979)\n",
      "('00617', 47538645.0, 1692937.0, 18.355, 0.654, 18.446889, -66.561154)\n",
      "('00622', 80724808.0, 19583150.0, 31.168, 7.561, 17.988103, -67.160357)\n",
      "Removed files from stage zip_geo_stage\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the zip_geo_mapping table with all columns\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE zip_geo_mapping (\n",
    "        GEOID STRING,\n",
    "        ALAND FLOAT,\n",
    "        AWATER FLOAT,\n",
    "        ALAND_SQMI FLOAT,\n",
    "        AWATER_SQMI FLOAT,\n",
    "        INTPTLAT FLOAT,\n",
    "        INTPTLONG FLOAT\n",
    "    );\n",
    "\"\"\")\n",
    "print(\"Table zip_geo_mapping with all columns created successfully.\")\n",
    "\n",
    "# Step 2: Create a stage to upload the CSV file\n",
    "stage_name = \"zip_geo_stage\"\n",
    "cs.execute(f\"CREATE OR REPLACE STAGE {stage_name}\")\n",
    "print(f\"Stage {stage_name} created successfully.\")\n",
    "\n",
    "# Step 3: Upload the CSV file to the Snowflake stage\n",
    "csv_file_path = output_csv_path\n",
    "\n",
    "# Perform the upload to Snowflake's stage\n",
    "put_query = f\"PUT file://{csv_file_path} @{stage_name}\"\n",
    "cs.execute(put_query)\n",
    "print(f\"Uploaded {os.path.basename(csv_file_path)} to Snowflake stage\")\n",
    "\n",
    "# Step 4: Copy the data from the stage into the zip_geo_mapping table\n",
    "copy_into_query = f\"\"\"\n",
    "COPY INTO zip_geo_mapping\n",
    "FROM @{stage_name}\n",
    "FILE_FORMAT = (\n",
    "    TYPE = 'CSV',\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',\n",
    "    SKIP_HEADER = 1\n",
    ")\n",
    "ON_ERROR = 'CONTINUE';\n",
    "\"\"\"\n",
    "cs.execute(copy_into_query)\n",
    "print(\"Data copied from stage to zip_geo_mapping table\")\n",
    "\n",
    "# Step 5: Verify the data loaded into the table\n",
    "cs.execute(\"SELECT * FROM zip_geo_mapping LIMIT 10;\")\n",
    "rows = cs.fetchall()\n",
    "print(\"Sample data from zip_geo_mapping:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Step 6: Clean up the stage (optional)\n",
    "cs.execute(f\"REMOVE @{stage_name}\")\n",
    "print(f\"Removed files from stage {stage_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to WEATHER__ENVIRONMENT database and CYBERSYN schema.\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     # Use the correct database name with double underscores\n",
    "#     cs.execute('USE DATABASE \"WEATHER__ENVIRONMENT\";')\n",
    "    \n",
    "#     # Check if CYBERSYN schema exists, if so use it\n",
    "#     cs.execute('USE SCHEMA \"CYBERSYN\";')\n",
    "    \n",
    "#     print(\"Switched to WEATHER__ENVIRONMENT database and CYBERSYN schema.\")\n",
    "    \n",
    "# except snowflake.connector.errors.ProgrammingError as e:\n",
    "#     print(f\"Error switching database/schema: {e}\")\n",
    "    \n",
    "#     # Debugging: List available schemas in the WEATHER__ENVIRONMENT database\n",
    "#     cs.execute('SHOW SCHEMAS IN DATABASE \"WEATHER__ENVIRONMENT\";')\n",
    "#     schemas = cs.fetchall()\n",
    "#     print(\"Available schemas in WEATHER__ENVIRONMENT:\")\n",
    "#     for schema in schemas:\n",
    "#         print(schema[1])  # Second column is the schema name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA_WEATHER_METRICS_TIMESERIES structure:\n",
      "('NOAA_WEATHER_STATION_ID', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Unique identifier for land surface weather stations in the Global Historical Climatology Network daily (GHCNd), comprised of the FIPS country code (first 2 characters), a network code (third character), and the actual station ID (the remaining 8 characters).', None, None)\n",
      "('VARIABLE', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'A unique identifier for a variable joinable to the timeseries table.', None, None)\n",
      "('VARIABLE_NAME', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Human-readable name for the variable.', None, None)\n",
      "('DATE', 'DATE', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Date associated with the value', None, None)\n",
      "('DATETIME', 'TIMESTAMP_NTZ(9)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Datetime associated with the value', None, None)\n",
      "('VALUE', 'NUMBER(38,6)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Value reported for the variable', None, None)\n",
      "('UNIT', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Unit of measurement for the reported value.', None, None)\n"
     ]
    }
   ],
   "source": [
    "# # Check the structure of the NOAA_WEATHER_METRICS_TIMESERIES view\n",
    "# cs.execute('DESCRIBE VIEW NOAA_WEATHER_METRICS_TIMESERIES;')\n",
    "# metrics_structure = cs.fetchall()\n",
    "# print(\"NOAA_WEATHER_METRICS_TIMESERIES structure:\")\n",
    "# for col in metrics_structure:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA_WEATHER_STATION_INDEX structure:\n",
      "('NOAA_WEATHER_STATION_ID', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Unique identifier for land surface weather stations in the Global Historical Climatology Network daily (GHCNd), comprised of the FIPS country code (first 2 characters), a network code (third character), and the actual station ID (the remaining 8 characters).', None, None)\n",
      "('NOAA_WEATHER_STATION_NAME', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Full name of the weather station.', None, None)\n",
      "('COUNTRY_GEO_ID', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, \"Cybersyn's unique identifier for a country, joinable to other datasets.\", None, None)\n",
      "('COUNTRY_NAME', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Full name of the country.', None, None)\n",
      "('STATE_GEO_ID', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, \"Cybersyn's unique identifier for a state, joinable to other datasets.\", None, None)\n",
      "('STATE_NAME', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Full name of the state.', None, None)\n",
      "('ZIP_GEO_ID', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, \"Cybersyn's unique identifier for a zip code, joinable to other datasets.\", None, None)\n",
      "('ZIP_NAME', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Full name of the zip code.', None, None)\n",
      "('LATITUDE', 'FLOAT', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Latitude of the land surface weather station (in decimal degrees).', None, None)\n",
      "('LONGITUDE', 'FLOAT', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Longitude of the land surface weather station (in decimal degrees).', None, None)\n",
      "('ELEVATION', 'FLOAT', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Elevation of the land surface weather station (in meters).', None, None)\n",
      "('WEATHER_STATION_NETWORK', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Source of the station ID, represented by the network code in that ID.', None, None)\n",
      "('ASSOCIATED_NETWORKS', 'ARRAY', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Array of networks the land surface weather station is part of.', None, None)\n",
      "('WORLD_METEOROLOGICAL_ORGANIZATION_ID', 'VARCHAR(16777216)', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'World Meteorological Organization (WMO) number for the land surface weather station, if available.', None, None)\n",
      "('SOURCE_DATA', 'ARRAY', 'COLUMN', 'Y', None, 'N', 'N', None, None, 'Array of sources for data points associated with the land surface weather station.', None, None)\n"
     ]
    }
   ],
   "source": [
    "# # Check the structure of the NOAA_WEATHER_STATION_INDEX view\n",
    "# cs.execute('DESCRIBE VIEW NOAA_WEATHER_STATION_INDEX;')\n",
    "# station_structure = cs.fetchall()\n",
    "# print(\"NOAA_WEATHER_STATION_INDEX structure:\")\n",
    "# for col in station_structure:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables in the schema:\n",
      "PURCHASES_DATA\n",
      "PURCHASE_ORDERS_AND_INVOICES\n",
      "RAWXMLDATA\n",
      "SUPPLIERINVOICES\n",
      "SUPPLIER_CASE\n",
      "SUPPLIER_TRANSACTIONS\n",
      "SUPPLIER_TRANSACTIONS_RAW\n",
      "ZIP_GEO_MAPPING\n"
     ]
    }
   ],
   "source": [
    "# # List all tables in the current schema to verify the existence of zip_geo_mapping\n",
    "# cs.execute(\"SHOW TABLES;\")\n",
    "# tables = cs.fetchall()\n",
    "\n",
    "# # Print the available tables\n",
    "# print(\"Available tables in the schema:\")\n",
    "# for table in tables:\n",
    "#     print(table[1])  # Second column is the table name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected query to map ZIP codes to the closest weather station\n",
    "haversine_query = \"\"\"\n",
    "WITH closest_stations AS (\n",
    "    SELECT\n",
    "        z.GEOID AS zip_code,\n",
    "        z.INTPTLAT AS zip_lat,\n",
    "        z.INTPTLONG AS zip_lon,\n",
    "        s.NOAA_WEATHER_STATION_ID,\n",
    "        s.LATITUDE AS station_lat,\n",
    "        s.LONGITUDE AS station_lon,\n",
    "        -- Haversine formula to calculate distance\n",
    "        (6371 * ACOS(\n",
    "            COS(RADIANS(z.INTPTLAT)) * COS(RADIANS(s.LATITUDE)) *\n",
    "            COS(RADIANS(s.LONGITUDE) - RADIANS(z.INTPTLONG)) +\n",
    "            SIN(RADIANS(z.INTPTLAT)) * SIN(RADIANS(s.LATITUDE))\n",
    "        )) AS distance_km,\n",
    "        ROW_NUMBER() OVER (PARTITION BY z.GEOID ORDER BY \n",
    "            (6371 * ACOS(\n",
    "                COS(RADIANS(z.INTPTLAT)) * COS(RADIANS(s.LATITUDE)) *\n",
    "                COS(RADIANS(s.LONGITUDE) - RADIANS(z.INTPTLONG)) +\n",
    "                SIN(RADIANS(z.INTPTLAT)) * SIN(RADIANS(s.LATITUDE))\n",
    "            )) ASC) AS rank\n",
    "    FROM FINAL_PROJECT_DB.FINAL_PROJECT_SCHEMA.ZIP_GEO_MAPPING z\n",
    "    JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX s\n",
    "    ON TRUE\n",
    ")\n",
    "-- Select the closest station for each ZIP code\n",
    "SELECT\n",
    "    zip_code,\n",
    "    NOAA_WEATHER_STATION_ID,\n",
    "    distance_km\n",
    "FROM closest_stations\n",
    "WHERE rank = 1;\n",
    "\"\"\"\n",
    "\n",
    "cs.execute(haversine_query)\n",
    "closest_station_data = cs.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [zip_code, NOAA_WEATHER_STATION_ID, closest_station_distance]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Convert the result to a Pandas DataFrame\n",
    "df_closest_station = pd.DataFrame(closest_station_data, columns=['zip_code', 'NOAA_WEATHER_STATION_ID', 'closest_station_distance'])\n",
    "print(df_closest_station.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to retrieve daily high temperatures for each ZIP code\n",
    "weather_query = \"\"\"\n",
    "WITH closest_stations AS (\n",
    "    -- Get the closest station for each ZIP code\n",
    "    SELECT zip_code, NOAA_WEATHER_STATION_ID\n",
    "    FROM FINAL_PROJECT_DB.FINAL_PROJECT_SCHEMA.ZIP_GEO_MAPPING z\n",
    "    JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX s\n",
    "    ON TRUE\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY z.GEOID ORDER BY \n",
    "    (6371 * ACOS(\n",
    "        COS(RADIANS(z.INTPTLAT)) * COS(RADIANS(s.LATITUDE)) *\n",
    "        COS(RADIANS(s.LONGITUDE) - RADIANS(z.INTPTLONG)) +\n",
    "        SIN(RADIANS(z.INTPTLAT)) * SIN(RADIANS(s.LATITUDE))\n",
    "    )) ASC) = 1\n",
    ")\n",
    "SELECT\n",
    "    cs.zip_code,\n",
    "    wm.DATE,\n",
    "    wm.VALUE AS high_temperature\n",
    "FROM closest_stations cs\n",
    "JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES wm\n",
    "ON cs.NOAA_WEATHER_STATION_ID = wm.NOAA_WEATHER_STATION_ID\n",
    "WHERE wm.VARIABLE_NAME = 'Temperature - Max';\n",
    "\"\"\"\n",
    "\n",
    "# Execute the weather query\n",
    "cs.execute(weather_query)\n",
    "weather_data = cs.fetchall()\n",
    "\n",
    "# Convert the result to a Pandas DataFrame for analysis\n",
    "df_weather = pd.DataFrame(weather_data, columns=['zip_code', 'date', 'high_temperature'])\n",
    "print(df_weather.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to create the supplier_zip_code_weather table\n",
    "create_table_query = \"\"\"\n",
    "CREATE OR REPLACE TABLE supplier_zip_code_weather AS\n",
    "WITH closest_stations AS (\n",
    "    -- Map ZIP code to the closest weather station\n",
    "    SELECT zip_code, NOAA_WEATHER_STATION_ID\n",
    "    FROM FINAL_PROJECT_DB.FINAL_PROJECT_SCHEMA.ZIP_GEO_MAPPING z\n",
    "    JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_STATION_INDEX s\n",
    "    ON TRUE\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY z.GEOID ORDER BY \n",
    "    (6371 * ACOS(\n",
    "        COS(RADIANS(z.INTPTLAT)) * COS(RADIANS(s.LATITUDE)) *\n",
    "        COS(RADIANS(s.LONGITUDE) - RADIANS(z.INTPTLONG)) +\n",
    "        SIN(RADIANS(z.INTPTLAT)) * SIN(RADIANS(s.LATITUDE))\n",
    "    )) ASC) = 1\n",
    ")\n",
    "-- Join with NOAA weather metrics to get high temperatures for each ZIP code\n",
    "SELECT\n",
    "    cs.zip_code,\n",
    "    wm.DATE,\n",
    "    wm.VALUE AS high_temperature\n",
    "FROM closest_stations cs\n",
    "JOIN WEATHER__ENVIRONMENT.CYBERSYN.NOAA_WEATHER_METRICS_TIMESERIES wm\n",
    "ON cs.NOAA_WEATHER_STATION_ID = wm.NOAA_WEATHER_STATION_ID\n",
    "WHERE wm.VARIABLE_NAME = 'Temperature - Max';\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to create the table\n",
    "cs.execute(create_table_query)\n",
    "print(\"Table 'supplier_zip_code_weather' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to check the first 10 rows of the table\n",
    "verify_query = \"SELECT * FROM supplier_zip_code_weather LIMIT 10;\"\n",
    "cs.execute(verify_query)\n",
    "rows = cs.fetchall()\n",
    "\n",
    "# Display the results\n",
    "for row in rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8:\n",
    "\n",
    "8. **Join** `purchase_orders_and_invoices`, `supplier_case`, and `supplier_zip_code_weather` based on zip codes and the transaction date. Only include transactions that have matching temperature readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
