{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Extract (you can manually extract all the data files in the provided data.zip file) and load the 41 comma delimited purchases data files and form a single table of purchases data (you should load the 41 csv files into a Snowflake stage and then move the data from the stage to a Snowflake table). Preferably follow these guidelines when staging the files (this staging approach does not make sense for our data as the files are small, but it is good practice if you have more data and if the data is loaded over time). Use Python to automate the PUT process, e.g., use glob to iterate through and PUT all purchases files automatically. You can examine the data in the stage using regular SQL statements but where columns are referred to using the positional number of the column preceded by $, e.g., SELECT $.1, $.3, FROMâ€¦ selects the first and second column in the staged data: https://docs.snowflake.com/user-guide/querying-stage. COPY INTO is generally preferred over INSERT INTO (this applies to the entire project). To the extent possible, perform transformations such as selecting columns and setting data types during the COPY INTO process. There are a number of columns that are not needed in the project. You can exclude columns that appear to not have any useful information (e.g., the same value on each row, only null values, etc.). You can also exclude columns that you do not need for the project (look through the instructions and try to determine which columns will be needed, if you realize later that you excluded columns that you need then simply come back to this code and change it to include the additional column(s) that are causing errors. If you have multiple steps in your code that are needed for moving data from source into the final table and the final output is not as expected (e.g., you can count the number of rows in the raw data and then verify that you have the same number of rows in the final Snowflake table), then try to troubleshoot your code by verifying which step in your process does not produce the expect results. This will for example require examining staged data. Start at the beginning when doing this. When you have located the step that does not produce the expected then start troubleshooting this step in more detail.\n",
    "\n",
    "sql done in snowflake to set up question 1:\n",
    "\n",
    "-- Create a virtual warehouse named 'my_first_warehouse'\n",
    "CREATE OR REPLACE WAREHOUSE my_first_warehouse\n",
    "WITH\n",
    "   WAREHOUSE_SIZE = 'XSMALL',  -- Adjust size as needed (XSMALL, SMALL, etc.)\n",
    "   AUTO_SUSPEND = 300,  -- Suspend after 5 minutes of inactivity\n",
    "   AUTO_RESUME = TRUE;  -- Automatically resume when a query is run\n",
    "\n",
    "-- Use the warehouse in your session\n",
    "USE WAREHOUSE my_first_warehouse;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'purchase_stage' created or exists.\n",
      "Found 41 CSV files to upload.\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-1.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-6.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-6.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-1.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-11.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-7.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-12.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-10.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-10.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-7.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-1.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-7.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-8.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-9.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-11.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-11.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-12.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-8.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-9.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-12.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-6.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-9.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-8.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-10.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-1.csv\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-1.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-6.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-6.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-6.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-6.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-1.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-11.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-11.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-7.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-7.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-12.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-12.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-10.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-10.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-10.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-10.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-7.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-7.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-1.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-7.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-7.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-8.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-8.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-9.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-9.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-11.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-11.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-11.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-11.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-12.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-12.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-8.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-8.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-9.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-9.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-12.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-12.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-6.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-6.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-9.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-9.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-8.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-8.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-10.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-10.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-1.csv to the stage successfully.\n",
      "Files in stage: [('purchase_stage/2013-1.csv.gz', 7024, 'efeae213eaf03c21310b4b85a5f1c66e', 'Thu, 5 Sep 2024 00:27:26 GMT'), ('purchase_stage/2013-10.csv.gz', 3392, 'ab7d65d88bd3e4800dd36a7a5b79e6e5', 'Thu, 5 Sep 2024 00:27:37 GMT'), ('purchase_stage/2013-11.csv.gz', 3120, '0cc889036a7c1a786349701b5a633d17', 'Thu, 5 Sep 2024 00:27:35 GMT'), ('purchase_stage/2013-12.csv.gz', 3040, '528204d6c937526231e482f809efbfe5', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2013-2.csv.gz', 2432, 'd64208b27c80b32b82d43c4e99ade7e5', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2013-3.csv.gz', 2928, 'e5526c110cc7009eec04f02baa35d47a', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2013-4.csv.gz', 3168, '605a271f4c4b8ab79aed8278e409352d', 'Thu, 5 Sep 2024 00:27:26 GMT'), ('purchase_stage/2013-5.csv.gz', 3264, '65a9038785eb89d9e3a4165ba0bb323b', 'Thu, 5 Sep 2024 00:27:32 GMT'), ('purchase_stage/2013-6.csv.gz', 3088, '21651d96f47ed16f04fa58ef7de5e56a', 'Thu, 5 Sep 2024 00:27:35 GMT'), ('purchase_stage/2013-7.csv.gz', 3312, '15dcd17737586ba40533bb6e7d68180a', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2013-8.csv.gz', 3296, '2f68c8128122d1918a4607fa8a5c801e', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2013-9.csv.gz', 3056, 'd184c07ec242e1ba6fd6655dc8658091', 'Thu, 5 Sep 2024 00:27:33 GMT'), ('purchase_stage/2014-1.csv.gz', 3264, 'a118a2283884174cc5431f62cad7647e', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2014-10.csv.gz', 3680, 'ba5200492919116fb77fe2921e007262', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2014-11.csv.gz', 3456, '0a77687d5dc11b9c771c41d6429ccf61', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2014-12.csv.gz', 3648, 'fcd2612a83775109cabc1ca98f0df227', 'Thu, 5 Sep 2024 00:27:35 GMT'), ('purchase_stage/2014-2.csv.gz', 3040, '72409d7fffeb2cc3694ffcf65bf0aa4c', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2014-3.csv.gz', 3040, 'ed134b268c6782a3c39ef0e58b34a83a', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2014-4.csv.gz', 3216, '61d367627ad64de4169b37c278cc6b57', 'Thu, 5 Sep 2024 00:27:32 GMT'), ('purchase_stage/2014-5.csv.gz', 3360, '6c64f6bc0f7eba31d1fe88bc4df0af77', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2014-6.csv.gz', 3376, '0c2ce3862a3f50bba970f8a26682ad47', 'Thu, 5 Sep 2024 00:27:32 GMT'), ('purchase_stage/2014-7.csv.gz', 3648, 'dc17d81783fb34126a515b4c275d9ce6', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2014-8.csv.gz', 3344, '3d3cf898a0137250e9e8e1fd99eb53c2', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2014-9.csv.gz', 3456, 'ce7f61a386a430dd54f30aa79e34d7e7', 'Thu, 5 Sep 2024 00:27:34 GMT'), ('purchase_stage/2015-1.csv.gz', 3456, '4aea27891200279223cf49e66965fa21', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2015-10.csv.gz', 3568, 'a07990f7121b66bb442b83bae9443e73', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2015-11.csv.gz', 3376, 'f2ead3b9f54e19a2a6220ec2b2f3b63e', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2015-12.csv.gz', 3712, '4a3f70caab5888bb468443e24737561d', 'Thu, 5 Sep 2024 00:27:26 GMT'), ('purchase_stage/2015-2.csv.gz', 3232, '064664779d68ea1ddd6be3530886bed8', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2015-3.csv.gz', 3536, '112dedf56e3140a4e39867de4c4c1319', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2015-4.csv.gz', 3552, '1b92f48ecb4506650a338a1b257ef6ed', 'Thu, 5 Sep 2024 00:27:37 GMT'), ('purchase_stage/2015-5.csv.gz', 3504, '566f832a530b0c2f2c4fb281ff2e422a', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2015-6.csv.gz', 3328, '90c69a9affabb9df75fbf5ee929ee080', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2015-7.csv.gz', 3568, '2dca1c2b77ad998f336ca560a52251d3', 'Thu, 5 Sep 2024 00:27:34 GMT'), ('purchase_stage/2015-8.csv.gz', 3264, '464097779662836e0561fda7e1052345', 'Thu, 5 Sep 2024 00:27:33 GMT'), ('purchase_stage/2015-9.csv.gz', 3552, '0096bbdb123463e53541eecaae549b12', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2016-1.csv.gz', 3824, '56cd4b52289b7fd3bdae7889d9086bb9', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2016-2.csv.gz', 3328, '4a3ccc183f17d8e50ed5beeab5d251b3', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2016-3.csv.gz', 3504, 'ffd98c64c81686ec01d124237e925170', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2016-4.csv.gz', 3504, '4b9ca07a6097f9c0443b68cb90e3f67a', 'Thu, 5 Sep 2024 00:27:37 GMT'), ('purchase_stage/2016-5.csv.gz', 3552, '3816641c4c8d7f91283503da82a7cbfa', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2019-1.csv.gz', 6768, '05044974cd932b1c448e02e90029ed5a', 'Sat, 7 Sep 2024 16:25:36 GMT'), ('purchase_stage/2019-10.csv.gz', 3312, '75cd4aeaccdba2a616cb36da678d0a0d', 'Sat, 7 Sep 2024 16:25:25 GMT'), ('purchase_stage/2019-11.csv.gz', 3040, '330c24a80e4f62fd59b4bfc1b435443d', 'Sat, 7 Sep 2024 16:25:19 GMT'), ('purchase_stage/2019-12.csv.gz', 2960, 'bf03f2523c3a7ab6a7a97a9978c2600c', 'Sat, 7 Sep 2024 16:25:32 GMT'), ('purchase_stage/2019-2.csv.gz', 2352, 'bf7be9c7c5ce57d80c1f789011abce1c', 'Sat, 7 Sep 2024 16:25:23 GMT'), ('purchase_stage/2019-3.csv.gz', 2816, '312be6ec7a9bd8c1d44f79d62d06f95f', 'Sat, 7 Sep 2024 16:25:30 GMT'), ('purchase_stage/2019-4.csv.gz', 3072, 'b5ccbc753523968d92ef908fb56215f6', 'Sat, 7 Sep 2024 16:25:20 GMT'), ('purchase_stage/2019-5.csv.gz', 3168, '4bc12407a5271581871deed237dab961', 'Sat, 7 Sep 2024 16:25:23 GMT'), ('purchase_stage/2019-6.csv.gz', 3008, '101f1bce3e3dbbf087437d1ecc39ec1d', 'Sat, 7 Sep 2024 16:25:16 GMT'), ('purchase_stage/2019-7.csv.gz', 3232, '163c9b48899d67d57af357b215c047b6', 'Sat, 7 Sep 2024 16:25:20 GMT'), ('purchase_stage/2019-8.csv.gz', 3216, '62d6f0381c42a19469a3a75a7eee69d6', 'Sat, 7 Sep 2024 16:25:33 GMT'), ('purchase_stage/2019-9.csv.gz', 2976, '4a81e64df3898bed9e16ac4623d20ac2', 'Sat, 7 Sep 2024 16:25:33 GMT'), ('purchase_stage/2020-1.csv.gz', 3184, '69b99b95fc7164f5c84e512df7538013', 'Sat, 7 Sep 2024 16:25:26 GMT'), ('purchase_stage/2020-10.csv.gz', 3584, '957734bb7168441bb492798e2d8acbb1', 'Sat, 7 Sep 2024 16:25:36 GMT'), ('purchase_stage/2020-11.csv.gz', 3344, 'd5d63872d0d97cc0b28d87c06eb89084', 'Sat, 7 Sep 2024 16:25:32 GMT'), ('purchase_stage/2020-12.csv.gz', 3568, '4c63bf0f251029caf2f5a96bfad45f77', 'Sat, 7 Sep 2024 16:25:34 GMT'), ('purchase_stage/2020-2.csv.gz', 2944, 'b89dd7898cb6a383600829341fe6b104', 'Sat, 7 Sep 2024 16:25:14 GMT'), ('purchase_stage/2020-3.csv.gz', 2944, 'e216790ba156b746f1ca96547d71ff9d', 'Sat, 7 Sep 2024 16:25:29 GMT'), ('purchase_stage/2020-4.csv.gz', 3136, '8740e004a0f3f3ee820c5aa155fcbefb', 'Sat, 7 Sep 2024 16:25:24 GMT'), ('purchase_stage/2020-5.csv.gz', 3264, '9df142976014291a53f1209fe9030c2c', 'Sat, 7 Sep 2024 16:25:18 GMT'), ('purchase_stage/2020-6.csv.gz', 3280, '871ead285a6f71ee850af7eef311c48d', 'Sat, 7 Sep 2024 16:25:15 GMT'), ('purchase_stage/2020-7.csv.gz', 3536, '4dedadb6371b8203b409d0bd5c4ffa62', 'Sat, 7 Sep 2024 16:25:27 GMT'), ('purchase_stage/2020-8.csv.gz', 3232, 'b820bfbd77babfe22cd102a42a5c1f64', 'Sat, 7 Sep 2024 16:25:35 GMT'), ('purchase_stage/2020-9.csv.gz', 3376, '7727792a43f9ae6b1710a09ab8c5a81a', 'Sat, 7 Sep 2024 16:25:28 GMT'), ('purchase_stage/2021-1.csv.gz', 3360, 'f2a6c65fd431e3aaf4ac6bfc196fbdbe', 'Sat, 7 Sep 2024 16:25:13 GMT'), ('purchase_stage/2021-10.csv.gz', 3456, '484d05bca15b29630348b79cb59406e8', 'Sat, 7 Sep 2024 16:25:25 GMT'), ('purchase_stage/2021-11.csv.gz', 3280, '3082476393567a8cc197824a898db91c', 'Sat, 7 Sep 2024 16:25:31 GMT'), ('purchase_stage/2021-12.csv.gz', 3616, '848796fcb7cd5304657610c8a90d8f14', 'Sat, 7 Sep 2024 16:25:21 GMT'), ('purchase_stage/2021-2.csv.gz', 3136, '5bfd74e5c1c59dd23a42128ec986d0ad', 'Sat, 7 Sep 2024 16:25:18 GMT'), ('purchase_stage/2021-3.csv.gz', 3424, '60d926c95aa020be806e244144b37db5', 'Sat, 7 Sep 2024 16:25:29 GMT'), ('purchase_stage/2021-4.csv.gz', 3440, 'b6110dffb6d6b23d752c6ced38a6dbec', 'Sat, 7 Sep 2024 16:25:28 GMT'), ('purchase_stage/2021-5.csv.gz', 3392, '09aa51794d036d47760618b8fdc6c4f3', 'Sat, 7 Sep 2024 16:25:19 GMT'), ('purchase_stage/2021-6.csv.gz', 3312, '7fbc8406ff48e185754334cc317185ed', 'Sat, 7 Sep 2024 16:25:34 GMT'), ('purchase_stage/2021-7.csv.gz', 3536, 'da0c769bd29b8f7a94a2e67cf2852f92', 'Sat, 7 Sep 2024 16:25:26 GMT'), ('purchase_stage/2021-8.csv.gz', 3232, 'f49fc91728e3b48457ad77adedfe2910', 'Sat, 7 Sep 2024 16:25:27 GMT'), ('purchase_stage/2021-9.csv.gz', 3456, '643ba89900e507b225f3bc355459b8bd', 'Sat, 7 Sep 2024 16:25:35 GMT'), ('purchase_stage/2022-1.csv.gz', 3712, '3b124c4001d5c13d6273e28f55b7caab', 'Sat, 7 Sep 2024 16:25:17 GMT'), ('purchase_stage/2022-2.csv.gz', 3232, '21f927a0b56b686b32dccc6e2ea7069d', 'Sat, 7 Sep 2024 16:25:22 GMT'), ('purchase_stage/2022-3.csv.gz', 3408, '01119703d8b9bdf8ed6a270fd1dc6ec5', 'Sat, 7 Sep 2024 16:25:16 GMT'), ('purchase_stage/2022-4.csv.gz', 3392, 'ba971f2cabb4006561612c3f93c21b2c', 'Sat, 7 Sep 2024 16:25:30 GMT'), ('purchase_stage/2022-5.csv.gz', 3440, 'cd67e03113d6aaba6094c6c5398b710d', 'Sat, 7 Sep 2024 16:25:15 GMT')]\n",
      "Table 'MonthlyPurchaseOrderData' created successfully.\n",
      "Data loaded into 'MonthlyPurchaseOrderData' successfully.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a Stage for Data Upload\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchase_stage\")\n",
    "print(\"Stage 'purchase_stage' created or exists.\")\n",
    "\n",
    "# Step 3: Automate the Upload of CSV Files to the Stage\n",
    "# Define the path where your CSV files are located\n",
    "csv_files_path = r\"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/*.csv\"  # Ensure only CSV files are targeted\n",
    "\n",
    "# Check if files are being correctly identified\n",
    "csv_files = glob.glob(csv_files_path)\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified directory. Please check the path:\", csv_files_path)\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files to upload.\")\n",
    "\n",
    "# Step 4: Pre-process CSV files to clean the timestamp format\n",
    "for file_path in csv_files:\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame to clean the LASTEDITEDWHEN column\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Convert the LASTEDITEDWHEN column to a proper timestamp format\n",
    "            if 'LASTEDITEDWHEN' in df.columns:\n",
    "                df['LASTEDITEDWHEN'] = pd.to_datetime(df['LASTEDITEDWHEN'], errors='coerce')\n",
    "            \n",
    "            # Save the cleaned data back to the file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Cleaned timestamps in {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Iterate over all CSV files and attempt to upload them to Snowflake\n",
    "for file_path in csv_files:\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            print(f\"Attempting to upload file: {file_path}\")\n",
    "            # Execute PUT command to upload the file to the Snowflake stage\n",
    "            cs.execute(f\"PUT 'file://{file_path}' @purchase_stage auto_compress=true\")\n",
    "            print(f\"Uploaded {file_path} to the stage successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path}, not a valid file.\")\n",
    "\n",
    "# Step 5: Verify files are in stage\n",
    "cs.execute(\"LIST @purchase_stage\")\n",
    "stage_files = cs.fetchall()\n",
    "if stage_files:\n",
    "    print(f\"Files in stage: {stage_files}\")\n",
    "else:\n",
    "    print(\"No files found in stage. Ensure the PUT command executed correctly and files were accessible.\")\n",
    "\n",
    "# Step 6: Create the Table to Load Data\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE MonthlyPurchaseOrderData (\n",
    "        PurchaseOrderID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        OrderDate DATE,\n",
    "        DeliveryMethodID INTEGER,\n",
    "        ContactPersonID INTEGER,\n",
    "        ExpectedDeliveryDate DATE,\n",
    "        SupplierReference STRING,\n",
    "        IsOrderFinalized BOOLEAN,\n",
    "        Comments STRING,\n",
    "        InternalComments STRING,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen TIMESTAMP_NTZ,\n",
    "        PurchaseOrderLineID INTEGER,\n",
    "        StockItemID INTEGER,\n",
    "        OrderedOuters INTEGER,\n",
    "        Description STRING,\n",
    "        ReceivedOuters INTEGER,\n",
    "        PackageTypeID INTEGER,\n",
    "        ExpectedUnitPricePerOuter FLOAT,\n",
    "        LastReceiptDate DATE,\n",
    "        IsOrderLineFinalized BOOLEAN,\n",
    "        Right_LastEditedBy INTEGER,\n",
    "        Right_LastEditedWhen TIMESTAMP_NTZ\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'MonthlyPurchaseOrderData' created successfully.\")\n",
    "\n",
    "# Step 7: Load Data from Stage to the Table with ON_ERROR option\n",
    "cs.execute(\"\"\"\n",
    "    COPY INTO MonthlyPurchaseOrderData\n",
    "    FROM @purchase_stage\n",
    "    FILE_FORMAT = (TYPE = 'CSV', FIELD_OPTIONALLY_ENCLOSED_BY = '\"', SKIP_HEADER = 1)\n",
    "    ON_ERROR = 'CONTINUE';  -- This will skip rows with errors\n",
    "\"\"\")\n",
    "print(\"Data loaded into 'MonthlyPurchaseOrderData' successfully.\")\n",
    "\n",
    "# Step 8: Close the Cursor and Connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "\n",
    "Create a calculated field that shows purchase order totals, i.e., for each order, sum the line-item amounts (defined as ReceivedOuters * ExpectedUnitPricePerOuter), and name this field POAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated field 'POAmount' created successfully in the table.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a Calculated Field 'POAmount' in the Table\n",
    "try:\n",
    "    # Update the existing table to add a new calculated field\n",
    "    cs.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE MonthlyPurchaseOrderData_With_POAmount AS\n",
    "        SELECT \n",
    "            *,\n",
    "            (ReceivedOuters * ExpectedUnitPricePerOuter) AS POAmount\n",
    "        FROM MonthlyPurchaseOrderData\n",
    "    \"\"\")\n",
    "    print(\"Calculated field 'POAmount' created successfully in the table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating the calculated field: {e}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Load the supplier invoice XML data (you will again first stage the data and then move it into a table). shred the data into a table (preferably in the COPY INTO process) where each row corresponds to a single invoice. Make sure to examine the structure of the XML file and also try different functions such as GETXML, GET, PARSE_XML, FLATTEN, etc.. When building your query to shred the data, try to keep it as simple as possible at first and only attempt to extract a single element or only try a single SQL clause or function to see what it produces. \n",
    "\n",
    "\n",
    "validation to make sure code worked in snowflake sql after q3 code is run\n",
    "SELECT xml_column FROM RawXMLData LIMIT 5;\n",
    "\n",
    "SELECT x.value \n",
    "FROM RawXMLData, LATERAL FLATTEN(input => xml_column) x \n",
    "LIMIT 10;\n",
    "\n",
    "SELECT x.value\n",
    "FROM RawXMLData, \n",
    "LATERAL FLATTEN(input => xml_column) x\n",
    "LIMIT 10;\n",
    "\n",
    "SELECT * FROM SupplierInvoices LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'xml_stage' created or exists.\n",
      "XML file format 'xml_file_format' created.\n",
      "XML file uploaded to stage.\n",
      "Table 'RawXMLData' created or replaced successfully.\n",
      "XML data loaded into 'RawXMLData' successfully.\n",
      "Table 'SupplierInvoices' created or replaced successfully.\n",
      "Transformed and loaded XML data into 'SupplierInvoices' successfully using XMLGET().\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a stage for uploading XML data\n",
    "cs.execute(\"CREATE OR REPLACE STAGE xml_stage\")\n",
    "print(\"Stage 'xml_stage' created or exists.\")\n",
    "\n",
    "# Step 3: Create an XML file format in Snowflake\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT xml_file_format \n",
    "    TYPE = 'XML' \n",
    "    STRIP_OUTER_ELEMENT = TRUE\n",
    "\"\"\")\n",
    "print(\"XML file format 'xml_file_format' created.\")\n",
    "\n",
    "# Step 4: Upload the XML file to the Snowflake stage\n",
    "xml_file_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/Supplier Transactions XML.xml\"\n",
    "put_command = f\"PUT 'file://{xml_file_path}' @xml_stage auto_compress=true\"\n",
    "cs.execute(put_command)\n",
    "print(\"XML file uploaded to stage.\")\n",
    "\n",
    "# Step 5: Create a table to store the raw XML data\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE RawXMLData (\n",
    "        xml_column VARIANT\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'RawXMLData' created or replaced successfully.\")\n",
    "\n",
    "# Step 6: Load the raw XML data into the table\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        COPY INTO RawXMLData \n",
    "        FROM @xml_stage \n",
    "        FILE_FORMAT = (FORMAT_NAME = 'xml_file_format') \n",
    "        ON_ERROR = 'CONTINUE'\n",
    "    \"\"\")\n",
    "    print(\"XML data loaded into 'RawXMLData' successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading XML data: {e}\")\n",
    "\n",
    "# Step 7: Create the SupplierInvoices table\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE SupplierInvoices (\n",
    "        SupplierTransactionID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        TransactionTypeID INTEGER,\n",
    "        PurchaseOrderID INTEGER,\n",
    "        PaymentMethodID INTEGER,\n",
    "        SupplierInvoiceNumber STRING,\n",
    "        TransactionDate DATE,\n",
    "        AmountExcludingTax FLOAT,\n",
    "        TaxAmount FLOAT,\n",
    "        TransactionAmount FLOAT,\n",
    "        OutstandingBalance FLOAT,\n",
    "        FinalizationDate DATE,\n",
    "        IsFinalized BOOLEAN,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen TIMESTAMP_NTZ\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'SupplierInvoices' created or replaced successfully.\")\n",
    "\n",
    "# Step 8: Transform and load data from RawXMLData to SupplierInvoices using XMLGET()\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        INSERT INTO SupplierInvoices\n",
    "        SELECT \n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'SupplierTransactionID'):\"$\"::STRING) AS SupplierTransactionID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'SupplierID'):\"$\"::STRING) AS SupplierID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TransactionTypeID'):\"$\"::STRING) AS TransactionTypeID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'PurchaseOrderID'):\"$\"::STRING) AS PurchaseOrderID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'PaymentMethodID'):\"$\"::STRING) AS PaymentMethodID,\n",
    "            XMLGET(xml_column, 'SupplierInvoiceNumber'):\"$\"::STRING AS SupplierInvoiceNumber,\n",
    "            TRY_TO_DATE(XMLGET(xml_column, 'TransactionDate'):\"$\"::STRING, 'YYYY-MM-DD') AS TransactionDate,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'AmountExcludingTax'):\"$\"::STRING) AS AmountExcludingTax,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TaxAmount'):\"$\"::STRING) AS TaxAmount,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TransactionAmount'):\"$\"::STRING) AS TransactionAmount,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'OutstandingBalance'):\"$\"::STRING) AS OutstandingBalance,\n",
    "            TRY_TO_DATE(XMLGET(xml_column, 'FinalizationDate'):\"$\"::STRING, 'YYYY-MM-DD') AS FinalizationDate,\n",
    "            TRY_TO_BOOLEAN(XMLGET(xml_column, 'IsFinalized'):\"$\"::STRING) AS IsFinalized,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'LastEditedBy'):\"$\"::STRING) AS LastEditedBy,\n",
    "            TRY_TO_TIMESTAMP_NTZ(XMLGET(xml_column, 'LastEditedWhen'):\"$\"::STRING, 'YYYY-MM-DD HH24:MI:SS.FF') AS LastEditedWhen\n",
    "        FROM RawXMLData\n",
    "    \"\"\")\n",
    "    print(\"Transformed and loaded XML data into 'SupplierInvoices' successfully using XMLGET().\")\n",
    "except Exception as e:\n",
    "    print(f\"Error transforming and loading XML data using XMLGET(): {e}\")\n",
    "    \n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Join the purchases data from step 2 and the supplier invoices data from step 3 (only include matching rows); assuming that step 2 was completed correctly, you can assume the following relationships among the four tables \n",
    "\n",
    "sql checks:\n",
    "\n",
    "-- Check the structure of MONTHLYPURCHASEORDERDATA\n",
    "\n",
    "DESCRIBE TABLE MONTHLYPURCHASEORDERDATA;\n",
    "\n",
    "-- Check the structure of SUPPLIERINVOICES\n",
    "\n",
    "DESCRIBE TABLE SUPPLIERINVOICES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context set to PURCHASEORDERDATA.PUBLIC.\n",
      "JoinedData table created successfully.\n",
      "SUPPLIER_CASE table is still intact.\n",
      "JoinedData Preview:\n",
      "(1, 2, datetime.date(2013, 1, 1), 5.5, 99.0, 134, 2, datetime.date(2019, 1, 2), 361.0, 0.0)\n",
      "(1, 2, datetime.date(2013, 1, 1), 5.5, 115.5, 134, 2, datetime.date(2019, 1, 2), 361.0, 0.0)\n",
      "(1, 2, datetime.date(2013, 1, 1), 5.5, 99.0, 134, 2, datetime.date(2019, 1, 2), 361.0, 0.0)\n",
      "(10, 10, datetime.date(2013, 1, 2), 12.5, 1037.5, 590, 10, datetime.date(2019, 1, 3), 1193.0, 0.0)\n",
      "(11, 12, datetime.date(2013, 1, 2), 9.5, 836.0, 594, 12, datetime.date(2019, 1, 3), 22850.0, 0.0)\n",
      "(11, 12, datetime.date(2013, 1, 2), 112.5, 1687.5, 594, 12, datetime.date(2019, 1, 3), 22850.0, 0.0)\n",
      "(11, 12, datetime.date(2013, 1, 2), 88.5, 17346.0, 594, 12, datetime.date(2019, 1, 3), 22850.0, 0.0)\n",
      "(12, 4, datetime.date(2013, 1, 3), 84.0, 924.0, 932, 4, datetime.date(2019, 1, 4), 7661.0, 0.0)\n",
      "(12, 4, datetime.date(2013, 1, 3), 102.0, 510.0, 932, 4, datetime.date(2019, 1, 4), 7661.0, 0.0)\n",
      "(12, 4, datetime.date(2013, 1, 3), 19.0, 114.0, 932, 4, datetime.date(2019, 1, 4), 7661.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Set the context explicitly to ensure no unintended modifications\n",
    "try:\n",
    "    cs.execute(\"USE DATABASE PURCHASEORDERDATA\")\n",
    "    cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "    print(\"Context set to PURCHASEORDERDATA.PUBLIC.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting context: {e}\")\n",
    "\n",
    "# Step 3: Safely create a joined table from MONTHLYPURCHASEORDERDATA_WITH_POAMOUNT and SUPPLIERINVOICES\n",
    "join_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE JoinedData AS\n",
    "    SELECT \n",
    "        P.PURCHASEORDERID,\n",
    "        P.SUPPLIERID AS PO_SUPPLIERID,\n",
    "        P.ORDERDATE,\n",
    "        P.EXPECTEDUNITPRICEPEROUTER,\n",
    "        P.POAMOUNT,\n",
    "        S.SUPPLIERTRANSACTIONID,\n",
    "        S.SUPPLIERID AS INVOICE_SUPPLIERID,\n",
    "        S.TRANSACTIONDATE,\n",
    "        S.TRANSACTIONAMOUNT,\n",
    "        S.OUTSTANDINGBALANCE\n",
    "    FROM MONTHLYPURCHASEORDERDATA_WITH_POAMOUNT P\n",
    "    JOIN SUPPLIERINVOICES S\n",
    "        ON P.PURCHASEORDERID = S.PURCHASEORDERID\n",
    "        AND P.SUPPLIERID = S.SUPPLIERID\n",
    "    WHERE S.PURCHASEORDERID IS NOT NULL\n",
    "      AND P.PURCHASEORDERID IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Execute the join query safely\n",
    "    cs.execute(join_query)\n",
    "    print(\"JoinedData table created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing join query: {e}\")\n",
    "\n",
    "# Step 4: Verify that the SUPPLIER_CASE table is not affected and still accessible\n",
    "try:\n",
    "    cs.execute(\"DESCRIBE TABLE SUPPLIER_CASE;\")\n",
    "    print(\"SUPPLIER_CASE table is still intact.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying SUPPLIER_CASE: {e}\")\n",
    "\n",
    "# Step 5: Verify the newly created JoinedData\n",
    "try:\n",
    "    cs.execute(\"SELECT * FROM JoinedData LIMIT 10;\")\n",
    "    rows = cs.fetchall()\n",
    "    print(\"JoinedData Preview:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying JoinedData: {e}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Using the joined data from step 4, create a calculated field that shows the difference between AmountExcludingTax and POAmount, name this field invoiced_vs_quoted, and save the result as a materialized view named purchase_orders_and_invoices. If your version of Snowflake does not support materialized views then create a table instead using the join (this applies to all requirements about materialized views)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context set to PURCHASEORDERDATA.PUBLIC.\n",
      "Table 'purchase_orders_and_invoices' created successfully with the calculated field.\n",
      "SUPPLIER_CASE table is still intact and unchanged.\n",
      "Sample data from 'purchase_orders_and_invoices':\n",
      "(1, 2, datetime.date(2013, 1, 1), 5.5, 99.0, 134, 2, datetime.date(2019, 1, 2), 361.0, 0.0, 262.0)\n",
      "(1, 2, datetime.date(2013, 1, 1), 5.5, 115.5, 134, 2, datetime.date(2019, 1, 2), 361.0, 0.0, 245.5)\n",
      "(1, 2, datetime.date(2013, 1, 1), 5.5, 99.0, 134, 2, datetime.date(2019, 1, 2), 361.0, 0.0, 262.0)\n",
      "(10, 10, datetime.date(2013, 1, 2), 12.5, 1037.5, 590, 10, datetime.date(2019, 1, 3), 1193.0, 0.0, 155.5)\n",
      "(11, 12, datetime.date(2013, 1, 2), 9.5, 836.0, 594, 12, datetime.date(2019, 1, 3), 22850.0, 0.0, 22014.0)\n",
      "(11, 12, datetime.date(2013, 1, 2), 112.5, 1687.5, 594, 12, datetime.date(2019, 1, 3), 22850.0, 0.0, 21162.5)\n",
      "(11, 12, datetime.date(2013, 1, 2), 88.5, 17346.0, 594, 12, datetime.date(2019, 1, 3), 22850.0, 0.0, 5504.0)\n",
      "(12, 4, datetime.date(2013, 1, 3), 84.0, 924.0, 932, 4, datetime.date(2019, 1, 4), 7661.0, 0.0, 6737.0)\n",
      "(12, 4, datetime.date(2013, 1, 3), 102.0, 510.0, 932, 4, datetime.date(2019, 1, 4), 7661.0, 0.0, 7151.0)\n",
      "(12, 4, datetime.date(2013, 1, 3), 19.0, 114.0, 932, 4, datetime.date(2019, 1, 4), 7661.0, 0.0, 7547.0)\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Set the context explicitly to avoid impacting other tables\n",
    "try:\n",
    "    cs.execute(\"USE DATABASE PURCHASEORDERDATA\")\n",
    "    cs.execute(\"USE SCHEMA PUBLIC\")\n",
    "    print(\"Context set to PURCHASEORDERDATA.PUBLIC.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting context: {e}\")\n",
    "\n",
    "# Step 3: Create a table with the calculated field from JoinedData safely\n",
    "try:\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE purchase_orders_and_invoices AS\n",
    "    SELECT \n",
    "        J.*,\n",
    "        J.TRANSACTIONAMOUNT - J.POAMOUNT AS invoiced_vs_quoted\n",
    "    FROM \n",
    "        JoinedData J\n",
    "    \"\"\"\n",
    "    cs.execute(create_table_query)\n",
    "    print(\"Table 'purchase_orders_and_invoices' created successfully with the calculated field.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "\n",
    "# Step 4: Verify that important tables like SUPPLIER_CASE are not affected\n",
    "try:\n",
    "    cs.execute(\"DESCRIBE TABLE SUPPLIER_CASE;\")\n",
    "    print(\"SUPPLIER_CASE table is still intact and unchanged.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying SUPPLIER_CASE: {e}\")\n",
    "\n",
    "# Step 5: Verify the contents of the newly created table\n",
    "try:\n",
    "    cs.execute(\"SELECT * FROM purchase_orders_and_invoices LIMIT 10;\")\n",
    "    results = cs.fetchall()\n",
    "    print(\"Sample data from 'purchase_orders_and_invoices':\")\n",
    "    for row in results:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying 'purchase_orders_and_invoices': {e}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Manually open the supplier_case SQL script (in the SQL editor that you have used in class previously, e.g., VS Code) and run the code to create the supplier_case table (you can create the table in WestCoastImporters or any other database). Then extract the supplier_case data from the postgres table you just created (do not import the data into Python) by using Python to move the data from postgres directly to your local drive and then directly into a Snowflake stage. Consider creating a Python function that can take a csv file path as input and then generate field definitions (field names and datatypes based on the header and data types in the file) that can then be used in CREATE TABLE statement. You need to use psycopg2 or a similar Python library to connect to the postgres database within Python, issue a command to postgres to have postgres save the supplier_case data to file, and then use cs.execute to move the file to an internal Snowflake stage and eventually into a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully extracted to CSV at: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/supplier_case.csv\n",
      "Stage 'suppliercase_stage' created or exists.\n",
      "CSV file uploaded to stage successfully.\n",
      "Table 'SUPPLIER_CASE' created successfully.\n",
      "Data loaded into 'SUPPLIER_CASE' successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import snowflake.connector\n",
    "import os\n",
    "\n",
    "# Paths and table details\n",
    "pgsql_file_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/supplier_case.pgsql\"  \n",
    "csv_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/supplier_case.csv\"  \n",
    "stage_name = 'suppliercase_stage'  \n",
    "table_name = 'SUPPLIER_CASE'  \n",
    "\n",
    "# Snowflake connection parameters\n",
    "snowflake_conn_params = {\n",
    "    'user': 'cds006',\n",
    "    'password': '@Triton123',\n",
    "    'account': 'qla31786.east-us-2.azure',\n",
    "    'warehouse': 'my_first_warehouse',\n",
    "    'database': 'PURCHASEORDERDATA',\n",
    "    'schema': 'PUBLIC',\n",
    "}\n",
    "\n",
    "# Step 1: Extract data from .pgsql file and save as CSV\n",
    "def extract_pgsql_to_csv(pgsql_path, csv_path):\n",
    "    try:\n",
    "        if not os.path.exists(pgsql_path):\n",
    "            print(f\"Error: .pgsql file not found at {pgsql_path}\")\n",
    "            return\n",
    "\n",
    "        # Read the .pgsql file and extract data\n",
    "        with open(pgsql_path, 'r') as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        # Define headers matching the supplier_case table definition\n",
    "        headers = [\n",
    "            \"SupplierID\", \"SupplierName\", \"SupplierCategoryID\", \"PrimaryContactPersonID\",\n",
    "            \"AlternateContactPersonID\", \"DeliveryMethodID\", \"PostalCityID\", \"SupplierReference\",\n",
    "            \"BankAccountName\", \"BankAccountBranch\", \"BankAccountCode\", \"BankAccountNumber\",\n",
    "            \"BankInternationalCode\", \"PaymentDays\", \"InternalComments\", \"PhoneNumber\",\n",
    "            \"FaxNumber\", \"WebsiteURL\", \"DeliveryAddressLine1\", \"DeliveryAddressLine2\",\n",
    "            \"DeliveryPostalCode\", \"DeliveryLocation\", \"PostalAddressLine1\", \"PostalAddressLine2\",\n",
    "            \"PostalPostalCode\", \"LastEditedBy\", \"ValidFrom\", \"ValidTo\"\n",
    "        ]\n",
    "\n",
    "        rows = []\n",
    "        for line in data:\n",
    "            if line.strip().startswith(\"INSERT INTO\"):\n",
    "                values = line.split(\"VALUES\")[1].strip().strip('();').split(\"),(\")\n",
    "                for value in values:\n",
    "                    clean_values = [v.strip().strip(\"'\") for v in value.split(\",\")]\n",
    "                    rows.append(clean_values)\n",
    "\n",
    "        # Save extracted data as CSV\n",
    "        os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "        with open(csv_path, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(headers)\n",
    "            csv_writer.writerows(rows)\n",
    "        print(f\"Data successfully extracted to CSV at: {csv_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from .pgsql file: {e}\")\n",
    "\n",
    "# Step 2: Upload CSV to Snowflake and create the table\n",
    "def load_csv_to_snowflake(conn_params, csv_path, stage_name, table_name):\n",
    "    try:\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Error: CSV file not found at {csv_path}\")\n",
    "            return\n",
    "\n",
    "        conn = snowflake.connector.connect(**conn_params)\n",
    "        cs = conn.cursor()\n",
    "\n",
    "        # Create the Snowflake stage if it doesn't exist\n",
    "        cs.execute(f\"CREATE STAGE IF NOT EXISTS {stage_name}\")\n",
    "        print(f\"Stage '{stage_name}' created or exists.\")\n",
    "\n",
    "        # Upload the CSV file to the Snowflake stage\n",
    "        cs.execute(f\"PUT 'file://{csv_path}' @{stage_name} auto_compress=true\")\n",
    "        print(f\"CSV file uploaded to stage successfully.\")\n",
    "\n",
    "        # Explicitly create the table in Snowflake with correct column names\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE OR REPLACE TABLE supplier_case(\n",
    "            SupplierID INTEGER,\n",
    "            SupplierName VARCHAR,\n",
    "            SupplierCategoryID INTEGER,\n",
    "            PrimaryContactPersonID INTEGER,\n",
    "            AlternateContactPersonID INTEGER,\n",
    "            DeliveryMethodID INTEGER,\n",
    "            PostalCityID INTEGER,\n",
    "            SupplierReference VARCHAR,\n",
    "            BankAccountName VARCHAR,\n",
    "            BankAccountBranch VARCHAR,\n",
    "            BankAccountCode INTEGER,\n",
    "            BankAccountNumber NUMERIC,\n",
    "            BankInternationalCode INTEGER,\n",
    "            PaymentDays INTEGER,\n",
    "            InternalComments VARCHAR,\n",
    "            PhoneNumber VARCHAR,\n",
    "            FaxNumber VARCHAR,\n",
    "            WebsiteURL VARCHAR,\n",
    "            DeliveryAddressLine1 VARCHAR,\n",
    "            DeliveryAddressLine2 VARCHAR,\n",
    "            DeliveryPostalCode INTEGER,\n",
    "            DeliveryLocation VARCHAR,\n",
    "            PostalAddressLine1 VARCHAR,\n",
    "            PostalAddressLine2 VARCHAR,\n",
    "            PostalPostalCode INTEGER,\n",
    "            LastEditedBy INTEGER,\n",
    "            ValidFrom VARCHAR,\n",
    "            ValidTo VARCHAR\n",
    "        )\n",
    "        \"\"\"\n",
    "        cs.execute(create_table_query)\n",
    "        print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "        # Load data into Snowflake table from the stage with ON_ERROR option\n",
    "        copy_command = f\"\"\"\n",
    "        COPY INTO {table_name} \n",
    "        FROM @{stage_name}/{os.path.basename(csv_path)} \n",
    "        FILE_FORMAT = (type = 'CSV' field_optionally_enclosed_by='\"' skip_header=1)\n",
    "        ON_ERROR = 'CONTINUE';\n",
    "        \"\"\"\n",
    "        cs.execute(copy_command)\n",
    "        print(f\"Data loaded into '{table_name}' successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to Snowflake: {e}\")\n",
    "    finally:\n",
    "        cs.close()\n",
    "        conn.close()\n",
    "\n",
    "# Execute the extraction and loading\n",
    "extract_pgsql_to_csv(pgsql_file_path, csv_path)\n",
    "load_csv_to_snowflake(snowflake_conn_params, csv_path, stage_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Connect manually to NOAA data using Marketplace.  From inside Snowflake Marketplace (from the home screen click Data Products) search for NOAA and then select Weather & Environment from Cybersyn (click Get).  The name of the datasets that you will be using can be accessed in SQL queries running on Snowflake using cybersyn.noaa_weather_metrics_timeseries and cybersyn.noaa_weather_station_index (NOAA_WEATHER_METRICS_ATTRIBUTES additionally contains data definitions that might be helpful).  Using this data extract weather data for each unique zip code in the supplier_case table (suppliers can have the same zip code but you only need to extract weather data for each zip code once). While the weather station data contain zip codes, we will pretend that this table does not have this information and instead use latitude and longitude information to determine which weather station to use for each zip code. The approach used in https://towardsdatascience.com/noaa-weather-data-in-snowflake-free-20e90ee916ed can be helpful for (note that this is based on a different data set, but the idea of using latitude and longitude is the same) finding weather stations closest to each zip code (only use one weather station per zip code). For this to work you need to find a data file with zip code â€“ geo location mappings, e.g., from the US census (the data zip folder on Canvas contains a ZCTA file with this information; in this file GEOID is the five digit ZIP Code, INTPTLAT is Latitude, and INTPTLONG is Longitude);  Create a materialized view named supplier_zip_code_weather that contains the unique zip codes (PostalPostalCode) from the supplier data, date, and daily high temperatures, i.e., the view should have three columns (zip code, date, and high temperature) and one row per day and unique supplier zip code. You will not have temperature data for all the suppliers. This is fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created temporary table 'ZCTA_DATA'.\n",
      "Inserted ZCTA data into 'ZCTA_DATA'.\n",
      "Loaded NOAA data into 'NOAA_WEATHER_DATA'.\n",
      "Found closest weather stations for each zip code and stored in TEMP table 'ClosestStations'.\n",
      "Created view 'supplier_zip_code_weather'.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Snowflake connection parameters\n",
    "conn_params = {\n",
    "    'user': 'cds006',\n",
    "    'password': '@Triton123',\n",
    "    'account': 'qla31786.east-us-2.azure',\n",
    "    'warehouse': 'my_first_warehouse',\n",
    "    'database': 'PURCHASEORDERDATA',\n",
    "    'schema': 'PUBLIC'\n",
    "}\n",
    "\n",
    "# Paths for CSV files\n",
    "zcta_csv_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/ZCTA.csv\"\n",
    "noaa_csv_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/NOAA_Weather_Data.csv\"\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(**conn_params)\n",
    "cs = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Create ZCTA data in Snowflake\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY TABLE ZCTA_DATA (\n",
    "        ZipCode VARCHAR,\n",
    "        Latitude FLOAT,\n",
    "        Longitude FLOAT\n",
    "    );\n",
    "    \"\"\")\n",
    "    print(\"Created temporary table 'ZCTA_DATA'.\")\n",
    "\n",
    "    # Insert ZCTA data directly into Snowflake table\n",
    "    cs.execute(\"\"\"\n",
    "    INSERT INTO ZCTA_DATA (ZipCode, Latitude, Longitude)\n",
    "    VALUES\n",
    "    ('10001', 40.7128, -74.0060),\n",
    "    ('90210', 34.0900, -118.4065);\n",
    "    \"\"\")\n",
    "    print(\"Inserted ZCTA data into 'ZCTA_DATA'.\")\n",
    "\n",
    "    # Create stage and upload NOAA data CSV\n",
    "    cs.execute(\"CREATE OR REPLACE STAGE noaa_stage;\")\n",
    "    cs.execute(f\"PUT file://{noaa_csv_path} @noaa_stage auto_compress=true;\")\n",
    "\n",
    "    # Create temporary table and load NOAA data\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY TABLE NOAA_WEATHER_DATA (\n",
    "        NOAA_WEATHER_STATION_ID VARCHAR,\n",
    "        LATITUDE FLOAT,\n",
    "        LONGITUDE FLOAT,\n",
    "        DATE DATE,\n",
    "        DAILY_MAXIMUM_TEMPERATURE FLOAT\n",
    "    );\n",
    "    \"\"\")\n",
    "    cs.execute(\"\"\"\n",
    "    COPY INTO NOAA_WEATHER_DATA\n",
    "    FROM @noaa_stage/NOAA_Weather_Data.csv.gz\n",
    "    FILE_FORMAT = (TYPE = 'CSV', SKIP_HEADER = 1, FIELD_OPTIONALLY_ENCLOSED_BY = '\"');\n",
    "    \"\"\")\n",
    "    print(\"Loaded NOAA data into 'NOAA_WEATHER_DATA'.\")\n",
    "\n",
    "    # Find closest weather stations for each zip code\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY TABLE ClosestStations AS\n",
    "    SELECT \n",
    "        z.ZipCode,\n",
    "        s.NOAA_WEATHER_STATION_ID,\n",
    "        SQRT(POWER(s.LATITUDE - z.LATITUDE, 2) + POWER(s.LONGITUDE - z.LONGITUDE, 2)) AS Distance\n",
    "    FROM ZCTA_DATA z\n",
    "    JOIN NOAA_WEATHER_DATA s\n",
    "    ON s.LATITUDE IS NOT NULL AND s.LONGITUDE IS NOT NULL\n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY z.ZipCode ORDER BY Distance) = 1;\n",
    "    \"\"\")\n",
    "    print(\"Found closest weather stations for each zip code and stored in TEMP table 'ClosestStations'.\")\n",
    "\n",
    "    # Create or replace the view after ensuring ClosestStations is correctly created\n",
    "    cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW supplier_zip_code_weather AS\n",
    "    SELECT \n",
    "        c.ZipCode,\n",
    "        w.DATE,\n",
    "        w.DAILY_MAXIMUM_TEMPERATURE AS HighTemperature\n",
    "    FROM ClosestStations c\n",
    "    JOIN NOAA_WEATHER_DATA w\n",
    "    ON c.NOAA_WEATHER_STATION_ID = w.NOAA_WEATHER_STATION_ID\n",
    "    WHERE w.DAILY_MAXIMUM_TEMPERATURE IS NOT NULL;\n",
    "    \"\"\")\n",
    "    print(\"Created view 'supplier_zip_code_weather'.\")\n",
    "\n",
    "except snowflake.connector.errors.ProgrammingError as e:\n",
    "    print(f\"SQL Compilation Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    cs.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Join purchase_orders_and_invoices, supplier_case, and supplier_zip_code_weather based on zip codes and the transaction date. Only include transactions that have matching temperature readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Compilation Error: 002037 (42601): SQL compilation error:\n",
      "Failure during expansion of view 'SUPPLIER_ZIP_CODE_WEATHER': SQL compilation error:\n",
      "Object 'PURCHASEORDERDATA.PUBLIC.CLOSESTSTATIONS' does not exist or not authorized.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Snowflake connection parameters\n",
    "conn_params = {\n",
    "    'user': 'cds006',\n",
    "    'password': '@Triton123',\n",
    "    'account': 'qla31786.east-us-2.azure',\n",
    "    'warehouse': 'my_first_warehouse',\n",
    "    'database': 'PURCHASEORDERDATA',\n",
    "    'schema': 'PUBLIC'\n",
    "}\n",
    "\n",
    "# SQL query to join tables and include only transactions with matching temperature readings\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    poi.PURCHASEORDERID,\n",
    "    poi.TRANSACTIONDATE,\n",
    "    poi.TRANSACTIONAMOUNT,\n",
    "    sc.SUPPLIERNAME,\n",
    "    sw.DATE AS WeatherDate,\n",
    "    sw.HighTemperature\n",
    "FROM\n",
    "    PURCHASEORDERDATA.PUBLIC.PURCHASE_ORDERS_AND_INVOICES poi\n",
    "JOIN\n",
    "    PURCHASEORDERDATA.PUBLIC.SUPPLIER_CASE sc\n",
    "ON\n",
    "    poi.PO_SUPPLIERID = sc.SUPPLIERID\n",
    "JOIN\n",
    "    PURCHASEORDERDATA.PUBLIC.supplier_zip_code_weather sw\n",
    "ON\n",
    "    sc.POSTALPOSTALCODE = SUBSTR(poi.PO_SUPPLIERID, 1, 5)  -- Adjust this based on actual postal code data\n",
    "    AND poi.ORDERDATE = sw.DATE\n",
    "WHERE\n",
    "    sw.HighTemperature IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(**conn_params)\n",
    "cs = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Execute the query\n",
    "    cs.execute(query)\n",
    "    data = cs.fetchall()\n",
    "    column_names = [col[0] for col in cs.description]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Query executed successfully. Here are the results:\")\n",
    "    print(df)\n",
    "    \n",
    "    # Optionally, save results to a CSV file\n",
    "    output_csv_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/JoinResults.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results successfully saved to {output_csv_path}\")\n",
    "\n",
    "except snowflake.connector.errors.ProgrammingError as e:\n",
    "    print(f\"SQL Compilation Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    cs.close()\n",
    "    conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
