{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Extract (you can manually extract all the data files in the provided data.zip file) and load the 41 comma delimited purchases data files and form a single table of purchases data (you should load the 41 csv files into a Snowflake stage and then move the data from the stage to a Snowflake table). Preferably follow these guidelines when staging the files (this staging approach does not make sense for our data as the files are small, but it is good practice if you have more data and if the data is loaded over time). Use Python to automate the PUT process, e.g., use glob to iterate through and PUT all purchases files automatically. You can examine the data in the stage using regular SQL statements but where columns are referred to using the positional number of the column preceded by $, e.g., SELECT $.1, $.3, FROMâ€¦ selects the first and second column in the staged data: https://docs.snowflake.com/user-guide/querying-stage. COPY INTO is generally preferred over INSERT INTO (this applies to the entire project). To the extent possible, perform transformations such as selecting columns and setting data types during the COPY INTO process. There are a number of columns that are not needed in the project. You can exclude columns that appear to not have any useful information (e.g., the same value on each row, only null values, etc.). You can also exclude columns that you do not need for the project (look through the instructions and try to determine which columns will be needed, if you realize later that you excluded columns that you need then simply come back to this code and change it to include the additional column(s) that are causing errors. If you have multiple steps in your code that are needed for moving data from source into the final table and the final output is not as expected (e.g., you can count the number of rows in the raw data and then verify that you have the same number of rows in the final Snowflake table), then try to troubleshoot your code by verifying which step in your process does not produce the expect results. This will for example require examining staged data. Start at the beginning when doing this. When you have located the step that does not produce the expected then start troubleshooting this step in more detail.\n",
    "\n",
    "sql done in snowflake to set up question 1:\n",
    "\n",
    "-- Create a virtual warehouse named 'my_first_warehouse'\n",
    "CREATE OR REPLACE WAREHOUSE my_first_warehouse\n",
    "WITH\n",
    "   WAREHOUSE_SIZE = 'XSMALL',  -- Adjust size as needed (XSMALL, SMALL, etc.)\n",
    "   AUTO_SUSPEND = 300,  -- Suspend after 5 minutes of inactivity\n",
    "   AUTO_RESUME = TRUE;  -- Automatically resume when a query is run\n",
    "\n",
    "-- Use the warehouse in your session\n",
    "USE WAREHOUSE my_first_warehouse;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'purchase_stage' created or exists.\n",
      "Found 41 CSV files to upload.\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-1.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-6.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-6.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-1.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-11.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-7.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-12.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-2.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-5.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-10.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-10.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-7.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-1.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-7.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-8.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-9.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-3.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-4.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-11.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-11.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-12.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-8.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-9.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-12.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-6.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-9.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-8.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-10.csv\n",
      "Cleaned timestamps in /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-1.csv\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-1.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-6.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-6.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-6.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-6.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-1.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-11.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-11.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-7.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-7.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-12.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-12.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-2.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-2.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-5.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-5.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-10.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-10.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-10.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-10.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-7.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-7.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-1.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-7.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-7.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-8.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-8.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-9.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-9.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-3.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-3.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-4.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2022-4.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-11.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-11.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-11.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-11.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-12.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-12.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-8.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-8.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-9.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-9.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-12.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-12.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-6.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-6.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-9.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2021-9.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-8.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-8.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-10.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2020-10.csv to the stage successfully.\n",
      "Attempting to upload file: /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-1.csv\n",
      "Uploaded /home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/2019-1.csv to the stage successfully.\n",
      "Files in stage: [('purchase_stage/2013-1.csv.gz', 7024, 'efeae213eaf03c21310b4b85a5f1c66e', 'Thu, 5 Sep 2024 00:27:26 GMT'), ('purchase_stage/2013-10.csv.gz', 3392, 'ab7d65d88bd3e4800dd36a7a5b79e6e5', 'Thu, 5 Sep 2024 00:27:37 GMT'), ('purchase_stage/2013-11.csv.gz', 3120, '0cc889036a7c1a786349701b5a633d17', 'Thu, 5 Sep 2024 00:27:35 GMT'), ('purchase_stage/2013-12.csv.gz', 3040, '528204d6c937526231e482f809efbfe5', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2013-2.csv.gz', 2432, 'd64208b27c80b32b82d43c4e99ade7e5', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2013-3.csv.gz', 2928, 'e5526c110cc7009eec04f02baa35d47a', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2013-4.csv.gz', 3168, '605a271f4c4b8ab79aed8278e409352d', 'Thu, 5 Sep 2024 00:27:26 GMT'), ('purchase_stage/2013-5.csv.gz', 3264, '65a9038785eb89d9e3a4165ba0bb323b', 'Thu, 5 Sep 2024 00:27:32 GMT'), ('purchase_stage/2013-6.csv.gz', 3088, '21651d96f47ed16f04fa58ef7de5e56a', 'Thu, 5 Sep 2024 00:27:35 GMT'), ('purchase_stage/2013-7.csv.gz', 3312, '15dcd17737586ba40533bb6e7d68180a', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2013-8.csv.gz', 3296, '2f68c8128122d1918a4607fa8a5c801e', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2013-9.csv.gz', 3056, 'd184c07ec242e1ba6fd6655dc8658091', 'Thu, 5 Sep 2024 00:27:33 GMT'), ('purchase_stage/2014-1.csv.gz', 3264, 'a118a2283884174cc5431f62cad7647e', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2014-10.csv.gz', 3680, 'ba5200492919116fb77fe2921e007262', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2014-11.csv.gz', 3456, '0a77687d5dc11b9c771c41d6429ccf61', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2014-12.csv.gz', 3648, 'fcd2612a83775109cabc1ca98f0df227', 'Thu, 5 Sep 2024 00:27:35 GMT'), ('purchase_stage/2014-2.csv.gz', 3040, '72409d7fffeb2cc3694ffcf65bf0aa4c', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2014-3.csv.gz', 3040, 'ed134b268c6782a3c39ef0e58b34a83a', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2014-4.csv.gz', 3216, '61d367627ad64de4169b37c278cc6b57', 'Thu, 5 Sep 2024 00:27:32 GMT'), ('purchase_stage/2014-5.csv.gz', 3360, '6c64f6bc0f7eba31d1fe88bc4df0af77', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2014-6.csv.gz', 3376, '0c2ce3862a3f50bba970f8a26682ad47', 'Thu, 5 Sep 2024 00:27:32 GMT'), ('purchase_stage/2014-7.csv.gz', 3648, 'dc17d81783fb34126a515b4c275d9ce6', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2014-8.csv.gz', 3344, '3d3cf898a0137250e9e8e1fd99eb53c2', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2014-9.csv.gz', 3456, 'ce7f61a386a430dd54f30aa79e34d7e7', 'Thu, 5 Sep 2024 00:27:34 GMT'), ('purchase_stage/2015-1.csv.gz', 3456, '4aea27891200279223cf49e66965fa21', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2015-10.csv.gz', 3568, 'a07990f7121b66bb442b83bae9443e73', 'Thu, 5 Sep 2024 00:27:27 GMT'), ('purchase_stage/2015-11.csv.gz', 3376, 'f2ead3b9f54e19a2a6220ec2b2f3b63e', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2015-12.csv.gz', 3712, '4a3f70caab5888bb468443e24737561d', 'Thu, 5 Sep 2024 00:27:26 GMT'), ('purchase_stage/2015-2.csv.gz', 3232, '064664779d68ea1ddd6be3530886bed8', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2015-3.csv.gz', 3536, '112dedf56e3140a4e39867de4c4c1319', 'Thu, 5 Sep 2024 00:27:36 GMT'), ('purchase_stage/2015-4.csv.gz', 3552, '1b92f48ecb4506650a338a1b257ef6ed', 'Thu, 5 Sep 2024 00:27:37 GMT'), ('purchase_stage/2015-5.csv.gz', 3504, '566f832a530b0c2f2c4fb281ff2e422a', 'Thu, 5 Sep 2024 00:27:30 GMT'), ('purchase_stage/2015-6.csv.gz', 3328, '90c69a9affabb9df75fbf5ee929ee080', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2015-7.csv.gz', 3568, '2dca1c2b77ad998f336ca560a52251d3', 'Thu, 5 Sep 2024 00:27:34 GMT'), ('purchase_stage/2015-8.csv.gz', 3264, '464097779662836e0561fda7e1052345', 'Thu, 5 Sep 2024 00:27:33 GMT'), ('purchase_stage/2015-9.csv.gz', 3552, '0096bbdb123463e53541eecaae549b12', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2016-1.csv.gz', 3824, '56cd4b52289b7fd3bdae7889d9086bb9', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2016-2.csv.gz', 3328, '4a3ccc183f17d8e50ed5beeab5d251b3', 'Thu, 5 Sep 2024 00:27:31 GMT'), ('purchase_stage/2016-3.csv.gz', 3504, 'ffd98c64c81686ec01d124237e925170', 'Thu, 5 Sep 2024 00:27:28 GMT'), ('purchase_stage/2016-4.csv.gz', 3504, '4b9ca07a6097f9c0443b68cb90e3f67a', 'Thu, 5 Sep 2024 00:27:37 GMT'), ('purchase_stage/2016-5.csv.gz', 3552, '3816641c4c8d7f91283503da82a7cbfa', 'Thu, 5 Sep 2024 00:27:29 GMT'), ('purchase_stage/2019-1.csv.gz', 6768, '05044974cd932b1c448e02e90029ed5a', 'Sat, 7 Sep 2024 16:25:36 GMT'), ('purchase_stage/2019-10.csv.gz', 3312, '75cd4aeaccdba2a616cb36da678d0a0d', 'Sat, 7 Sep 2024 16:25:25 GMT'), ('purchase_stage/2019-11.csv.gz', 3040, '330c24a80e4f62fd59b4bfc1b435443d', 'Sat, 7 Sep 2024 16:25:19 GMT'), ('purchase_stage/2019-12.csv.gz', 2960, 'bf03f2523c3a7ab6a7a97a9978c2600c', 'Sat, 7 Sep 2024 16:25:32 GMT'), ('purchase_stage/2019-2.csv.gz', 2352, 'bf7be9c7c5ce57d80c1f789011abce1c', 'Sat, 7 Sep 2024 16:25:23 GMT'), ('purchase_stage/2019-3.csv.gz', 2816, '312be6ec7a9bd8c1d44f79d62d06f95f', 'Sat, 7 Sep 2024 16:25:30 GMT'), ('purchase_stage/2019-4.csv.gz', 3072, 'b5ccbc753523968d92ef908fb56215f6', 'Sat, 7 Sep 2024 16:25:20 GMT'), ('purchase_stage/2019-5.csv.gz', 3168, '4bc12407a5271581871deed237dab961', 'Sat, 7 Sep 2024 16:25:23 GMT'), ('purchase_stage/2019-6.csv.gz', 3008, '101f1bce3e3dbbf087437d1ecc39ec1d', 'Sat, 7 Sep 2024 16:25:16 GMT'), ('purchase_stage/2019-7.csv.gz', 3232, '163c9b48899d67d57af357b215c047b6', 'Sat, 7 Sep 2024 16:25:20 GMT'), ('purchase_stage/2019-8.csv.gz', 3216, '62d6f0381c42a19469a3a75a7eee69d6', 'Sat, 7 Sep 2024 16:25:33 GMT'), ('purchase_stage/2019-9.csv.gz', 2976, '4a81e64df3898bed9e16ac4623d20ac2', 'Sat, 7 Sep 2024 16:25:33 GMT'), ('purchase_stage/2020-1.csv.gz', 3184, '69b99b95fc7164f5c84e512df7538013', 'Sat, 7 Sep 2024 16:25:26 GMT'), ('purchase_stage/2020-10.csv.gz', 3584, '957734bb7168441bb492798e2d8acbb1', 'Sat, 7 Sep 2024 16:25:36 GMT'), ('purchase_stage/2020-11.csv.gz', 3344, 'd5d63872d0d97cc0b28d87c06eb89084', 'Sat, 7 Sep 2024 16:25:32 GMT'), ('purchase_stage/2020-12.csv.gz', 3568, '4c63bf0f251029caf2f5a96bfad45f77', 'Sat, 7 Sep 2024 16:25:34 GMT'), ('purchase_stage/2020-2.csv.gz', 2944, 'b89dd7898cb6a383600829341fe6b104', 'Sat, 7 Sep 2024 16:25:14 GMT'), ('purchase_stage/2020-3.csv.gz', 2944, 'e216790ba156b746f1ca96547d71ff9d', 'Sat, 7 Sep 2024 16:25:29 GMT'), ('purchase_stage/2020-4.csv.gz', 3136, '8740e004a0f3f3ee820c5aa155fcbefb', 'Sat, 7 Sep 2024 16:25:24 GMT'), ('purchase_stage/2020-5.csv.gz', 3264, '9df142976014291a53f1209fe9030c2c', 'Sat, 7 Sep 2024 16:25:18 GMT'), ('purchase_stage/2020-6.csv.gz', 3280, '871ead285a6f71ee850af7eef311c48d', 'Sat, 7 Sep 2024 16:25:15 GMT'), ('purchase_stage/2020-7.csv.gz', 3536, '4dedadb6371b8203b409d0bd5c4ffa62', 'Sat, 7 Sep 2024 16:25:27 GMT'), ('purchase_stage/2020-8.csv.gz', 3232, 'b820bfbd77babfe22cd102a42a5c1f64', 'Sat, 7 Sep 2024 16:25:35 GMT'), ('purchase_stage/2020-9.csv.gz', 3376, '7727792a43f9ae6b1710a09ab8c5a81a', 'Sat, 7 Sep 2024 16:25:28 GMT'), ('purchase_stage/2021-1.csv.gz', 3360, 'f2a6c65fd431e3aaf4ac6bfc196fbdbe', 'Sat, 7 Sep 2024 16:25:13 GMT'), ('purchase_stage/2021-10.csv.gz', 3456, '484d05bca15b29630348b79cb59406e8', 'Sat, 7 Sep 2024 16:25:25 GMT'), ('purchase_stage/2021-11.csv.gz', 3280, '3082476393567a8cc197824a898db91c', 'Sat, 7 Sep 2024 16:25:31 GMT'), ('purchase_stage/2021-12.csv.gz', 3616, '848796fcb7cd5304657610c8a90d8f14', 'Sat, 7 Sep 2024 16:25:21 GMT'), ('purchase_stage/2021-2.csv.gz', 3136, '5bfd74e5c1c59dd23a42128ec986d0ad', 'Sat, 7 Sep 2024 16:25:18 GMT'), ('purchase_stage/2021-3.csv.gz', 3424, '60d926c95aa020be806e244144b37db5', 'Sat, 7 Sep 2024 16:25:29 GMT'), ('purchase_stage/2021-4.csv.gz', 3440, 'b6110dffb6d6b23d752c6ced38a6dbec', 'Sat, 7 Sep 2024 16:25:28 GMT'), ('purchase_stage/2021-5.csv.gz', 3392, '09aa51794d036d47760618b8fdc6c4f3', 'Sat, 7 Sep 2024 16:25:19 GMT'), ('purchase_stage/2021-6.csv.gz', 3312, '7fbc8406ff48e185754334cc317185ed', 'Sat, 7 Sep 2024 16:25:34 GMT'), ('purchase_stage/2021-7.csv.gz', 3536, 'da0c769bd29b8f7a94a2e67cf2852f92', 'Sat, 7 Sep 2024 16:25:26 GMT'), ('purchase_stage/2021-8.csv.gz', 3232, 'f49fc91728e3b48457ad77adedfe2910', 'Sat, 7 Sep 2024 16:25:27 GMT'), ('purchase_stage/2021-9.csv.gz', 3456, '643ba89900e507b225f3bc355459b8bd', 'Sat, 7 Sep 2024 16:25:35 GMT'), ('purchase_stage/2022-1.csv.gz', 3712, '3b124c4001d5c13d6273e28f55b7caab', 'Sat, 7 Sep 2024 16:25:17 GMT'), ('purchase_stage/2022-2.csv.gz', 3232, '21f927a0b56b686b32dccc6e2ea7069d', 'Sat, 7 Sep 2024 16:25:22 GMT'), ('purchase_stage/2022-3.csv.gz', 3408, '01119703d8b9bdf8ed6a270fd1dc6ec5', 'Sat, 7 Sep 2024 16:25:16 GMT'), ('purchase_stage/2022-4.csv.gz', 3392, 'ba971f2cabb4006561612c3f93c21b2c', 'Sat, 7 Sep 2024 16:25:30 GMT'), ('purchase_stage/2022-5.csv.gz', 3440, 'cd67e03113d6aaba6094c6c5398b710d', 'Sat, 7 Sep 2024 16:25:15 GMT')]\n",
      "Table 'MonthlyPurchaseOrderData' created successfully.\n",
      "Data loaded into 'MonthlyPurchaseOrderData' successfully.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a Stage for Data Upload\n",
    "cs.execute(\"CREATE STAGE IF NOT EXISTS purchase_stage\")\n",
    "print(\"Stage 'purchase_stage' created or exists.\")\n",
    "\n",
    "# Step 3: Automate the Upload of CSV Files to the Stage\n",
    "# Define the path where your CSV files are located\n",
    "csv_files_path = r\"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/Monthly PO Data/*.csv\"  # Ensure only CSV files are targeted\n",
    "\n",
    "# Check if files are being correctly identified\n",
    "csv_files = glob.glob(csv_files_path)\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified directory. Please check the path:\", csv_files_path)\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files to upload.\")\n",
    "\n",
    "# Step 4: Pre-process CSV files to clean the timestamp format\n",
    "for file_path in csv_files:\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            # Load the CSV file into a DataFrame to clean the LASTEDITEDWHEN column\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Convert the LASTEDITEDWHEN column to a proper timestamp format\n",
    "            if 'LASTEDITEDWHEN' in df.columns:\n",
    "                df['LASTEDITEDWHEN'] = pd.to_datetime(df['LASTEDITEDWHEN'], errors='coerce')\n",
    "            \n",
    "            # Save the cleaned data back to the file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"Cleaned timestamps in {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Iterate over all CSV files and attempt to upload them to Snowflake\n",
    "for file_path in csv_files:\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            print(f\"Attempting to upload file: {file_path}\")\n",
    "            # Execute PUT command to upload the file to the Snowflake stage\n",
    "            cs.execute(f\"PUT 'file://{file_path}' @purchase_stage auto_compress=true\")\n",
    "            print(f\"Uploaded {file_path} to the stage successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path}, not a valid file.\")\n",
    "\n",
    "# Step 5: Verify files are in stage\n",
    "cs.execute(\"LIST @purchase_stage\")\n",
    "stage_files = cs.fetchall()\n",
    "if stage_files:\n",
    "    print(f\"Files in stage: {stage_files}\")\n",
    "else:\n",
    "    print(\"No files found in stage. Ensure the PUT command executed correctly and files were accessible.\")\n",
    "\n",
    "# Step 6: Create the Table to Load Data\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE MonthlyPurchaseOrderData (\n",
    "        PurchaseOrderID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        OrderDate DATE,\n",
    "        DeliveryMethodID INTEGER,\n",
    "        ContactPersonID INTEGER,\n",
    "        ExpectedDeliveryDate DATE,\n",
    "        SupplierReference STRING,\n",
    "        IsOrderFinalized BOOLEAN,\n",
    "        Comments STRING,\n",
    "        InternalComments STRING,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen TIMESTAMP_NTZ,\n",
    "        PurchaseOrderLineID INTEGER,\n",
    "        StockItemID INTEGER,\n",
    "        OrderedOuters INTEGER,\n",
    "        Description STRING,\n",
    "        ReceivedOuters INTEGER,\n",
    "        PackageTypeID INTEGER,\n",
    "        ExpectedUnitPricePerOuter FLOAT,\n",
    "        LastReceiptDate DATE,\n",
    "        IsOrderLineFinalized BOOLEAN,\n",
    "        Right_LastEditedBy INTEGER,\n",
    "        Right_LastEditedWhen TIMESTAMP_NTZ\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'MonthlyPurchaseOrderData' created successfully.\")\n",
    "\n",
    "# Step 7: Load Data from Stage to the Table with ON_ERROR option\n",
    "cs.execute(\"\"\"\n",
    "    COPY INTO MonthlyPurchaseOrderData\n",
    "    FROM @purchase_stage\n",
    "    FILE_FORMAT = (TYPE = 'CSV', FIELD_OPTIONALLY_ENCLOSED_BY = '\"', SKIP_HEADER = 1)\n",
    "    ON_ERROR = 'CONTINUE';  -- This will skip rows with errors\n",
    "\"\"\")\n",
    "print(\"Data loaded into 'MonthlyPurchaseOrderData' successfully.\")\n",
    "\n",
    "# Step 8: Close the Cursor and Connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "\n",
    "Create a calculated field that shows purchase order totals, i.e., for each order, sum the line-item amounts (defined as ReceivedOuters * ExpectedUnitPricePerOuter), and name this field POAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated field 'POAmount' created successfully in the table.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a Calculated Field 'POAmount' in the Table\n",
    "try:\n",
    "    # Update the existing table to add a new calculated field\n",
    "    cs.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE MonthlyPurchaseOrderData_With_POAmount AS\n",
    "        SELECT \n",
    "            *,\n",
    "            (ReceivedOuters * ExpectedUnitPricePerOuter) AS POAmount\n",
    "        FROM MonthlyPurchaseOrderData\n",
    "    \"\"\")\n",
    "    print(\"Calculated field 'POAmount' created successfully in the table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating the calculated field: {e}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Load the supplier invoice XML data (you will again first stage the data and then move it into a table). shred the data into a table (preferably in the COPY INTO process) where each row corresponds to a single invoice. Make sure to examine the structure of the XML file and also try different functions such as GETXML, GET, PARSE_XML, FLATTEN, etc.. When building your query to shred the data, try to keep it as simple as possible at first and only attempt to extract a single element or only try a single SQL clause or function to see what it produces. \n",
    "\n",
    "\n",
    "validation to make sure code worked in snowflake sql after q3 code is run\n",
    "SELECT xml_column FROM RawXMLData LIMIT 5;\n",
    "\n",
    "SELECT x.value \n",
    "FROM RawXMLData, LATERAL FLATTEN(input => xml_column) x \n",
    "LIMIT 10;\n",
    "\n",
    "SELECT x.value\n",
    "FROM RawXMLData, \n",
    "LATERAL FLATTEN(input => xml_column) x\n",
    "LIMIT 10;\n",
    "\n",
    "SELECT * FROM SupplierInvoices LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'xml_stage' created or exists.\n",
      "XML file format 'xml_file_format' created.\n",
      "XML file uploaded to stage.\n",
      "Table 'RawXMLData' created or replaced successfully.\n",
      "XML data loaded into 'RawXMLData' successfully.\n",
      "Table 'SupplierInvoices' created or replaced successfully.\n",
      "Transformed and loaded XML data into 'SupplierInvoices' successfully using XMLGET().\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a stage for uploading XML data\n",
    "cs.execute(\"CREATE OR REPLACE STAGE xml_stage\")\n",
    "print(\"Stage 'xml_stage' created or exists.\")\n",
    "\n",
    "# Step 3: Create an XML file format in Snowflake\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT xml_file_format \n",
    "    TYPE = 'XML' \n",
    "    STRIP_OUTER_ELEMENT = TRUE\n",
    "\"\"\")\n",
    "print(\"XML file format 'xml_file_format' created.\")\n",
    "\n",
    "# Step 4: Upload the XML file to the Snowflake stage\n",
    "xml_file_path = \"/home/jovyan/464/SQLETLSnowflake/CaseData/Data/Supplier Transactions XML.xml\"\n",
    "put_command = f\"PUT 'file://{xml_file_path}' @xml_stage auto_compress=true\"\n",
    "cs.execute(put_command)\n",
    "print(\"XML file uploaded to stage.\")\n",
    "\n",
    "# Step 5: Create a table to store the raw XML data\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE RawXMLData (\n",
    "        xml_column VARIANT\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'RawXMLData' created or replaced successfully.\")\n",
    "\n",
    "# Step 6: Load the raw XML data into the table\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        COPY INTO RawXMLData \n",
    "        FROM @xml_stage \n",
    "        FILE_FORMAT = (FORMAT_NAME = 'xml_file_format') \n",
    "        ON_ERROR = 'CONTINUE'\n",
    "    \"\"\")\n",
    "    print(\"XML data loaded into 'RawXMLData' successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading XML data: {e}\")\n",
    "\n",
    "# Step 7: Create the SupplierInvoices table\n",
    "cs.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE SupplierInvoices (\n",
    "        SupplierTransactionID INTEGER,\n",
    "        SupplierID INTEGER,\n",
    "        TransactionTypeID INTEGER,\n",
    "        PurchaseOrderID INTEGER,\n",
    "        PaymentMethodID INTEGER,\n",
    "        SupplierInvoiceNumber STRING,\n",
    "        TransactionDate DATE,\n",
    "        AmountExcludingTax FLOAT,\n",
    "        TaxAmount FLOAT,\n",
    "        TransactionAmount FLOAT,\n",
    "        OutstandingBalance FLOAT,\n",
    "        FinalizationDate DATE,\n",
    "        IsFinalized BOOLEAN,\n",
    "        LastEditedBy INTEGER,\n",
    "        LastEditedWhen TIMESTAMP_NTZ\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"Table 'SupplierInvoices' created or replaced successfully.\")\n",
    "\n",
    "# Step 8: Transform and load data from RawXMLData to SupplierInvoices using XMLGET()\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        INSERT INTO SupplierInvoices\n",
    "        SELECT \n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'SupplierTransactionID'):\"$\"::STRING) AS SupplierTransactionID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'SupplierID'):\"$\"::STRING) AS SupplierID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TransactionTypeID'):\"$\"::STRING) AS TransactionTypeID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'PurchaseOrderID'):\"$\"::STRING) AS PurchaseOrderID,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'PaymentMethodID'):\"$\"::STRING) AS PaymentMethodID,\n",
    "            XMLGET(xml_column, 'SupplierInvoiceNumber'):\"$\"::STRING AS SupplierInvoiceNumber,\n",
    "            TRY_TO_DATE(XMLGET(xml_column, 'TransactionDate'):\"$\"::STRING, 'YYYY-MM-DD') AS TransactionDate,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'AmountExcludingTax'):\"$\"::STRING) AS AmountExcludingTax,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TaxAmount'):\"$\"::STRING) AS TaxAmount,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'TransactionAmount'):\"$\"::STRING) AS TransactionAmount,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'OutstandingBalance'):\"$\"::STRING) AS OutstandingBalance,\n",
    "            TRY_TO_DATE(XMLGET(xml_column, 'FinalizationDate'):\"$\"::STRING, 'YYYY-MM-DD') AS FinalizationDate,\n",
    "            TRY_TO_BOOLEAN(XMLGET(xml_column, 'IsFinalized'):\"$\"::STRING) AS IsFinalized,\n",
    "            TRY_TO_NUMBER(XMLGET(xml_column, 'LastEditedBy'):\"$\"::STRING) AS LastEditedBy,\n",
    "            TRY_TO_TIMESTAMP_NTZ(XMLGET(xml_column, 'LastEditedWhen'):\"$\"::STRING, 'YYYY-MM-DD HH24:MI:SS.FF') AS LastEditedWhen\n",
    "        FROM RawXMLData\n",
    "    \"\"\")\n",
    "    print(\"Transformed and loaded XML data into 'SupplierInvoices' successfully using XMLGET().\")\n",
    "except Exception as e:\n",
    "    print(f\"Error transforming and loading XML data using XMLGET(): {e}\")\n",
    "    \n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Join the purchases data from step 2 and the supplier invoices data from step 3 (only include matching rows); assuming that step 2 was completed correctly, you can assume the following relationships among the four tables \n",
    "\n",
    "sql checks:\n",
    "\n",
    "-- Check the structure of MONTHLYPURCHASEORDERDATA\n",
    "\n",
    "DESCRIBE TABLE MONTHLYPURCHASEORDERDATA;\n",
    "\n",
    "-- Check the structure of SUPPLIERINVOICES\n",
    "\n",
    "DESCRIBE TABLE SUPPLIERINVOICES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JoinedData table created successfully.\n",
      "JoinedData Preview:\n",
      "(100, 4, datetime.date(2013, 2, 26), 84.0, 7476.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 84.0, 8988.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 84.0, 2184.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 96.0, 5952.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 90.0, 2160.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 96.0, 9792.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0)\n",
      "(101, 7, datetime.date(2013, 2, 26), 170.0, 3740.0, 12883, 7, datetime.date(2019, 2, 27), 4301.0, 0.0)\n",
      "(102, 4, datetime.date(2013, 2, 27), 84.0, 7308.0, 13077, 4, datetime.date(2019, 2, 28), 41400.0, 0.0)\n",
      "(102, 4, datetime.date(2013, 2, 27), 84.0, 8820.0, 13077, 4, datetime.date(2019, 2, 28), 41400.0, 0.0)\n",
      "(102, 4, datetime.date(2013, 2, 27), 84.0, 1848.0, 13077, 4, datetime.date(2019, 2, 28), 41400.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Join the Purchases and SupplierInvoices tables\n",
    "join_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE JoinedData AS\n",
    "    SELECT \n",
    "        P.PURCHASEORDERID,\n",
    "        P.SUPPLIERID AS PO_SUPPLIERID,\n",
    "        P.ORDERDATE,\n",
    "        P.EXPECTEDUNITPRICEPEROUTER,\n",
    "        P.POAMOUNT,\n",
    "        S.SUPPLIERTRANSACTIONID,\n",
    "        S.SUPPLIERID AS INVOICE_SUPPLIERID,\n",
    "        S.TRANSACTIONDATE,\n",
    "        S.TRANSACTIONAMOUNT,\n",
    "        S.OUTSTANDINGBALANCE\n",
    "    FROM MONTHLYPURCHASEORDERDATA_WITH_POAMOUNT P\n",
    "    JOIN SUPPLIERINVOICES S\n",
    "        ON P.PURCHASEORDERID = S.PURCHASEORDERID\n",
    "        AND P.SUPPLIERID = S.SUPPLIERID\n",
    "    WHERE S.PURCHASEORDERID IS NOT NULL\n",
    "      AND P.PURCHASEORDERID IS NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Execute the join query\n",
    "    cs.execute(join_query)\n",
    "    print(\"JoinedData table created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing join query: {e}\")\n",
    "\n",
    "# Step 3: Verify the joined data\n",
    "try:\n",
    "    cs.execute(\"SELECT * FROM JoinedData LIMIT 10;\")\n",
    "    rows = cs.fetchall()\n",
    "    print(\"JoinedData Preview:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying JoinedData: {e}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Using the joined data from step 4, create a calculated field that shows the difference between AmountExcludingTax and POAmount, name this field invoiced_vs_quoted, and save the result as a materialized view named purchase_orders_and_invoices. If your version of Snowflake does not support materialized views then create a table instead using the join (this applies to all requirements about materialized views)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'purchase_orders_and_invoices' created successfully with the calculated field.\n",
      "Sample data from 'purchase_orders_and_invoices':\n",
      "(100, 4, datetime.date(2013, 2, 26), 84.0, 7476.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0, 34559.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 84.0, 8988.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0, 33047.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 84.0, 2184.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0, 39851.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 96.0, 5952.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0, 36083.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 90.0, 2160.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0, 39875.0)\n",
      "(100, 4, datetime.date(2013, 2, 26), 96.0, 9792.0, 12881, 4, datetime.date(2019, 2, 27), 42035.0, 0.0, 32243.0)\n",
      "(101, 7, datetime.date(2013, 2, 26), 170.0, 3740.0, 12883, 7, datetime.date(2019, 2, 27), 4301.0, 0.0, 561.0)\n",
      "(102, 4, datetime.date(2013, 2, 27), 84.0, 7308.0, 13077, 4, datetime.date(2019, 2, 28), 41400.0, 0.0, 34092.0)\n",
      "(102, 4, datetime.date(2013, 2, 27), 84.0, 8820.0, 13077, 4, datetime.date(2019, 2, 28), 41400.0, 0.0, 32580.0)\n",
      "(102, 4, datetime.date(2013, 2, 27), 84.0, 1848.0, 13077, 4, datetime.date(2019, 2, 28), 41400.0, 0.0, 39552.0)\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Step 1: Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user='cds006',\n",
    "    password='@Triton123',\n",
    "    account='qla31786.east-us-2.azure',\n",
    "    warehouse='my_first_warehouse',\n",
    "    database='PURCHASEORDERDATA',\n",
    "    schema='PUBLIC'\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Step 2: Create a calculated field and table from JoinedData\n",
    "try:\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE purchase_orders_and_invoices AS\n",
    "    SELECT \n",
    "        J.*,\n",
    "        J.TRANSACTIONAMOUNT - J.POAMOUNT AS invoiced_vs_quoted\n",
    "    FROM \n",
    "        JoinedData J\n",
    "    \"\"\"\n",
    "    cs.execute(create_table_query)\n",
    "    print(\"Table 'purchase_orders_and_invoices' created successfully with the calculated field.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "\n",
    "# Step 3: Verify the created table\n",
    "try:\n",
    "    cs.execute(\"SELECT * FROM purchase_orders_and_invoices LIMIT 10;\")\n",
    "    results = cs.fetchall()\n",
    "    print(\"Sample data from 'purchase_orders_and_invoices':\")\n",
    "    for row in results:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying 'purchase_orders_and_invoices': {e}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Manually open the supplier_case SQL script (in the SQL editor that you have used in class previously, e.g., VS Code) and run the code to create the supplier_case table (you can create the table in WestCoastImporters or any other database). Then extract the supplier_case data from the postgres table you just created (do not import the data into Python) by using Python to move the data from postgres directly to your local drive and then directly into a Snowflake stage. Consider creating a Python function that can take a csv file path as input and then generate field definitions (field names and datatypes based on the header and data types in the file) that can then be used in CREATE TABLE statement. You need to use psycopg2 or a similar Python library to connect to the postgres database within Python, issue a command to postgres to have postgres save the supplier_case data to file, and then use cs.execute to move the file to an internal Snowflake stage and eventually into a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error exporting data: could not translate host name \"your_postgres_host\" to address: Name or service not known\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'cursor' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUPPLIER_CASE\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Table name in Snowflake\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Execute steps\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[43mextract_supplier_case_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostgres_conn_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m load_csv_to_snowflake(snowflake_conn_params, csv_path, stage_name, table_name)\n",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m, in \u001b[0;36mextract_supplier_case_to_csv\u001b[0;34m(postgres_conn_params, csv_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError exporting data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     22\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'cursor' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Extract supplier_case data from PostgreSQL and save it to CSV\n",
    "def extract_supplier_case_to_csv(postgres_conn_params, csv_path):\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(**postgres_conn_params)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Export supplier_case data to a CSV file\n",
    "        export_query = \"COPY supplier_case TO STDOUT WITH CSV HEADER\"\n",
    "        with open(csv_path, 'w') as f:\n",
    "            cursor.copy_expert(export_query, f)\n",
    "        print(f\"Data exported to {csv_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Step 2: Generate CREATE TABLE statement based on the extracted CSV file\n",
    "def generate_create_table_statement(csv_path, table_name):\n",
    "    # Read the CSV to infer data types\n",
    "    df = pd.read_csv(csv_path, nrows=100)  # Read a subset of rows to infer schema\n",
    "    columns = df.columns\n",
    "    col_types = df.dtypes\n",
    "\n",
    "    # Map pandas dtypes to Snowflake SQL data types\n",
    "    dtype_mapping = {\n",
    "        'int64': 'NUMBER',\n",
    "        'float64': 'FLOAT',\n",
    "        'object': 'STRING',\n",
    "        'bool': 'BOOLEAN',\n",
    "        'datetime64[ns]': 'TIMESTAMP_NTZ',\n",
    "    }\n",
    "\n",
    "    # Generate CREATE TABLE statement\n",
    "    column_defs = []\n",
    "    for col, dtype in zip(columns, col_types):\n",
    "        snowflake_dtype = dtype_mapping.get(str(dtype), 'STRING')  # Default to STRING if type unknown\n",
    "        column_defs.append(f\"{col.upper()} {snowflake_dtype}\")\n",
    "\n",
    "    create_table_stmt = f\"CREATE OR REPLACE TABLE {table_name} (\\n\" + \",\\n\".join(column_defs) + \"\\n);\"\n",
    "    return create_table_stmt\n",
    "\n",
    "# Step 3: Load the CSV into Snowflake by uploading to a stage and executing the COPY command\n",
    "def load_csv_to_snowflake(snowflake_conn_params, csv_path, stage_name, table_name):\n",
    "    conn = snowflake.connector.connect(**snowflake_conn_params)\n",
    "    cs = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Create the Snowflake stage if it doesn't exist\n",
    "        cs.execute(f\"CREATE STAGE IF NOT EXISTS {stage_name}\")\n",
    "        print(f\"Stage '{stage_name}' created or exists.\")\n",
    "\n",
    "        # Upload CSV file to Snowflake stage\n",
    "        put_command = f\"PUT 'file://{csv_path}' @{stage_name} auto_compress=true\"\n",
    "        cs.execute(put_command)\n",
    "        print(f\"Uploaded {csv_path} to the stage successfully.\")\n",
    "\n",
    "        # Generate and execute CREATE TABLE statement based on the CSV file structure\n",
    "        create_table_stmt = generate_create_table_statement(csv_path, table_name)\n",
    "        cs.execute(create_table_stmt)\n",
    "        print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "        # Load data into the Snowflake table from the stage\n",
    "        copy_command = f\"\"\"\n",
    "            COPY INTO {table_name}\n",
    "            FROM @{stage_name}/{os.path.basename(csv_path)}\n",
    "            FILE_FORMAT = (type = 'CSV' field_optionally_enclosed_by='\"' skip_header=1);\n",
    "        \"\"\"\n",
    "        cs.execute(copy_command)\n",
    "        print(f\"Data loaded into '{table_name}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data to Snowflake: {e}\")\n",
    "    finally:\n",
    "        cs.close()\n",
    "        conn.close()\n",
    "\n",
    "# Define connection parameters and file paths\n",
    "postgres_conn_params = {\n",
    "    'dbname': 'your_postgres_db',\n",
    "    'user': 'your_postgres_user',\n",
    "    'password': 'your_postgres_password',\n",
    "    'host': 'your_postgres_host',\n",
    "    'port': '5432',\n",
    "}\n",
    "\n",
    "snowflake_conn_params = {\n",
    "    'user': 'cds006',\n",
    "    'password': '@Triton123',\n",
    "    'account': 'qla31786.east-us-2.azure',\n",
    "    'warehouse': 'my_first_warehouse',\n",
    "    'database': 'PURCHASEORDERDATA',\n",
    "    'schema': 'PUBLIC',\n",
    "}\n",
    "\n",
    "# Define paths\n",
    "csv_path = '/home/jovyan/464/SQLETLSnowflake/CaseData/Data/supplier_case.csv'  # Local path for extracted data\n",
    "stage_name = 'supplier_stage'  # Snowflake stage name\n",
    "table_name = 'SUPPLIER_CASE'  # Table name in Snowflake\n",
    "\n",
    "# Execute steps\n",
    "extract_supplier_case_to_csv(postgres_conn_params, csv_path)\n",
    "load_csv_to_snowflake(snowflake_conn_params, csv_path, stage_name, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Connect manually to NOAA data using Marketplace.  From inside Snowflake Marketplace (from the home screen click Data Products) search for NOAA and then select Weather & Environment from Cybersyn (click Get).  The name of the datasets that you will be using can be accessed in SQL queries running on Snowflake using cybersyn.noaa_weather_metrics_timeseries and cybersyn.noaa_weather_station_index (NOAA_WEATHER_METRICS_ATTRIBUTES additionally contains data definitions that might be helpful).  Using this data extract weather data for each unique zip code in the supplier_case table (suppliers can have the same zip code but you only need to extract weather data for each zip code once). While the weather station data contain zip codes, we will pretend that this table does not have this information and instead use latitude and longitude information to determine which weather station to use for each zip code. The approach used in https://towardsdatascience.com/noaa-weather-data-in-snowflake-free-20e90ee916ed can be helpful for (note that this is based on a different data set, but the idea of using latitude and longitude is the same) finding weather stations closest to each zip code (only use one weather station per zip code). For this to work you need to find a data file with zip code â€“ geo location mappings, e.g., from the US census (the data zip folder on Canvas contains a ZCTA file with this information; in this file GEOID is the five digit ZIP Code, INTPTLAT is Latitude, and INTPTLONG is Longitude);  Create a materialized view named supplier_zip_code_weather that contains the unique zip codes (PostalPostalCode) from the supplier data, date, and daily high temperatures, i.e., the view should have three columns (zip code, date, and high temperature) and one row per day and unique supplier zip code. You will not have temperature data for all the suppliers. This is fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Join purchase_orders_and_invoices, supplier_case, and supplier_zip_code_weather based on zip codes and the transaction date. Only include transactions that have matching temperature readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
